{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the dataset\n",
    "with open('../data/dataset.json', 'r') as f:\n",
    "    dataset = json.load(f)\n",
    "    positives = dataset['positives']\n",
    "    negatives = dataset['negatives']\n",
    "\n",
    "# Shuffling the data\n",
    "a = [(i, 0) for i in negatives]\n",
    "b = [(i, 1) for i in positives]\n",
    "combined = a + b\n",
    "np.random.shuffle(combined)\n",
    "shuffled = list(zip(*combined))\n",
    "text_X = shuffled[0]\n",
    "labels = shuffled[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# characters level tf-idf\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram_chars.fit(text_X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(text_X, labels, test_size=0.2, random_state=42)\n",
    "xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(X_train) \n",
    "xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(X_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/research_data/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time:  0:01:44.285093\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "import datetime\n",
    "\n",
    "train_start = datetime.datetime.now()\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(xtrain_tfidf_ngram_chars, y_train)\n",
    "train_end = datetime.datetime.now()\n",
    "print(\"Training time: \", (train_end - train_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of logistic regression classifier on test set: 0.88\n"
     ]
    }
   ],
   "source": [
    "y_pred = logreg.predict(xvalid_tfidf_ngram_chars)\n",
    "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(xvalid_tfidf_ngram_chars, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[258746  28785]\n",
      " [ 41944 255866]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The result is telling us that we have 258746+255866 correct predictions and 28785+41944  incorrect predictions."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Compute precision, recall, F-measure and support\n",
    "To quote from Scikit Learn:\n",
    "The precision is the ratio tp / (tp + fp) where tp is the number of true positives and fp the number of false positives. The precision is intuitively the ability of the classifier to not label a sample as positive if it is negative.\n",
    "The recall is the ratio tp / (tp + fn) where tp is the number of true positives and fn the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples.\n",
    "The F-beta score can be interpreted as a weighted harmonic mean of the precision and recall, where an F-beta score reaches its best value at 1 and worst score at 0.\n",
    "The F-beta score weights the recall more than the precision by a factor of beta. beta = 1.0 means recall and precision are equally important.\n",
    "The support is the number of occurrences of each class in y_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = ensemble.RandomForestClassifier()\n",
    "clf.fit(xtrain_tfidf_ngram_chars, y_train)\n",
    "accuracy = clf.score(xvalid_tfidf_ngram_chars, y_test)\n",
    "print(\"RF, WordLevel TF-IDF: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RF, WordLevel TF-IDF:  0.8980850478609904"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
