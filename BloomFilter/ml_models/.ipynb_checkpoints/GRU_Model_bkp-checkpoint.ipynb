{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "import datetime\n",
    "from collections import Counter\n",
    "import matplotlib as pyplot\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Tensorflow and keras imports\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "tf.compat.v1.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data():\n",
    "    maxlen = 50\n",
    "    embedding_size = 50\n",
    "    \n",
    "    # importing the glove embeddings path \n",
    "    embeddings_path = '../data/glove.6B.50d-char.txt'\n",
    "    \n",
    "    # Indexing character vectors using glove word vectors\n",
    "    embedding_vectors = {}\n",
    "    with open(embeddings_path, 'r') as f:\n",
    "        for line in f:\n",
    "            line_split = line.strip().split(\" \")\n",
    "            vec = np.array(line_split[1:], dtype=float)\n",
    "            char = line_split[0]\n",
    "            embedding_vectors[char] = vec\n",
    "#     print('Found %s char vectors.' % len(embedding_vectors))\n",
    "    \n",
    "    # loading the dataset\n",
    "    with open('../data/dataset.json', 'r') as f:\n",
    "        dataset = json.load(f)\n",
    "        positives = dataset['positives']\n",
    "        negatives = dataset['negatives']\n",
    "    \n",
    "    # dividing the dataset to make small models\n",
    "    data_fraction = 0.5\n",
    "    positives = positives[:int(data_fraction * len(positives))]\n",
    "    negatives = negatives[:int(data_fraction * len(negatives))]\n",
    "    \n",
    "    # Dividing the negatives dataset between train, dev and test\n",
    "    negatives_train = negatives[0: int(len(negatives) * .8)]\n",
    "    negatives_dev = negatives[int(len(negatives) * .8): int(len(negatives) * .9)]\n",
    "    negatives_test = negatives[int(len(negatives) * .9): ]\n",
    "    print(\"Split sizes:\")\n",
    "    print(len(positives), len(negatives_train), len(negatives_dev), len(negatives_test))\n",
    "    \n",
    "    # Shuffling the data\n",
    "    a = [(i, 0) for i in negatives_train]\n",
    "    b = [(i, 1) for i in positives]\n",
    "    combined = a + b\n",
    "    random.shuffle(combined)\n",
    "    shuffled = list(zip(*combined))\n",
    "    text_X = shuffled[0]\n",
    "    labels = shuffled[1]\n",
    "    \n",
    "    # tokenizing the input url's\n",
    "    tk = keras.preprocessing.text.Tokenizer(char_level=True)\n",
    "    tk.fit_on_texts(text_X)\n",
    "    \n",
    "    # List the vocabulary\n",
    "    word_index = tk.word_index\n",
    "    vocab_size = len(word_index) + 1\n",
    "    \n",
    "    # integer encode the documents\n",
    "    sequences = tk.texts_to_sequences(text_X)\n",
    "\n",
    "    # pad documents to a max length of 4 words\n",
    "    data = keras.preprocessing.sequence.pad_sequences(sequences, maxlen=maxlen) # by default the padding is post.\n",
    "    labels = np.asarray(labels)\n",
    "    \n",
    "    # Dividing the dataset into train and test.\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # split the training data into a training set and a validation set\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "    \n",
    "    # create a weight matrix for words in training docs\n",
    "    embedding_matrix = np.zeros((vocab_size, 50))\n",
    "    for char, i in word_index.items():\n",
    "        embedding_vector = embedding_vectors.get(char)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    \n",
    "    # PCA Embedding dimension\n",
    "    pca_embedding_dim = 16\n",
    "    pca = PCA(n_components = pca_embedding_dim)\n",
    "    pca.fit(embedding_matrix[1:])\n",
    "    embedding_matrix_pca = np.array(pca.transform(embedding_matrix[1:]))\n",
    "    embedding_matrix_pca = np.insert(embedding_matrix_pca, 0, 0, axis=0)\n",
    "    print(\"PCA matrix created\")\n",
    "    \n",
    "    return (X_train, X_test, y_train, y_test, X_val, y_val, embedding_matrix_pca, maxlen, vocab_size,\n",
    "            positives, negatives_train, negatives_dev, negatives_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define GRU Model\n",
    "\n",
    "##### Defining embedding layer\n",
    "    Defining the embedding layer as the first layer of the model and using the original embedding dimension and not the reduced dimensions calculated using PCA. \n",
    "\n",
    "1. Use weights= embedding_matrix, if using the original embedding size else,\n",
    "2. Use weights= embedding_matrix_pca if using PCA.\n",
    "\n",
    "##### Defining a GRU layer with necessary arguments\n",
    "\n",
    "1. argument_1 = gru_size\n",
    "2. argument_2 = return sequences is False if there is no second gru layer, True otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(embedding_matrix, vocab_size, maxlen):\n",
    "    model = keras.Sequential([\n",
    "            keras.layers.Embedding(vocab_size, 16, input_length=50, weights=[embedding_matrix]),\n",
    "            keras.layers.GRU(16, return_sequences = False),\n",
    "            keras.layers.Dense(8, activation='relu'),\n",
    "            keras.layers.Dense(1, activation = 'sigmoid')\n",
    "    ])\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(lr = 0.001, decay = 0.0001)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    earlyStopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=1, min_delta = 0.001)\n",
    "    file_path = '../model_weights/keras_weights_GRU.hdf5'\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(filepath=file_path, verbose=1, save_best_only=True)\n",
    "    callbacks_list = [earlyStopping, checkpoint]\n",
    "    print(model.summary())\n",
    "    return model, callbacks_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split sizes:\n",
      "745589 574210 71776 71777\n",
      "PCA matrix created\n",
      "Data Preprocessing time: 0:00:35.468132\n"
     ]
    }
   ],
   "source": [
    "before_dataset = datetime.datetime.now()\n",
    "(X_train, X_test, y_train, y_test, X_val, y_val, embedding_matrix, maxlen, vocab_size, \n",
    "    positives, negatives_train, negatives_dev, negatives_test) = data()\n",
    "after_dataset = datetime.datetime.now()\n",
    "delta_dataset = after_dataset - before_dataset\n",
    "print(\"Data Preprocessing time:\", delta_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 50, 16)            624       \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, 16)                1632      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 2,401\n",
      "Trainable params: 2,401\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 950255 samples, validate on 105584 samples\n",
      "Epoch 1/50\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.54705, saving model to ../model_weights/keras_weights_GRU.hdf5\n",
      "950255/950255 - 42s - loss: 0.5792 - accuracy: 0.6737 - val_loss: 0.5471 - val_accuracy: 0.7000\n",
      "Epoch 2/50\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.54705 to 0.49151, saving model to ../model_weights/keras_weights_GRU.hdf5\n",
      "950255/950255 - 40s - loss: 0.5136 - accuracy: 0.7253 - val_loss: 0.4915 - val_accuracy: 0.7446\n",
      "Epoch 3/50\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.49151 to 0.46928, saving model to ../model_weights/keras_weights_GRU.hdf5\n",
      "950255/950255 - 39s - loss: 0.4795 - accuracy: 0.7532 - val_loss: 0.4693 - val_accuracy: 0.7611\n",
      "Epoch 4/50\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.46928 to 0.45620, saving model to ../model_weights/keras_weights_GRU.hdf5\n",
      "950255/950255 - 40s - loss: 0.4621 - accuracy: 0.7651 - val_loss: 0.4562 - val_accuracy: 0.7691\n",
      "Epoch 5/50\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.45620 to 0.44522, saving model to ../model_weights/keras_weights_GRU.hdf5\n",
      "950255/950255 - 39s - loss: 0.4504 - accuracy: 0.7726 - val_loss: 0.4452 - val_accuracy: 0.7756\n",
      "Epoch 6/50\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.44522 to 0.43677, saving model to ../model_weights/keras_weights_GRU.hdf5\n",
      "950255/950255 - 40s - loss: 0.4405 - accuracy: 0.7790 - val_loss: 0.4368 - val_accuracy: 0.7812\n",
      "Epoch 7/50\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.43677 to 0.42884, saving model to ../model_weights/keras_weights_GRU.hdf5\n",
      "950255/950255 - 39s - loss: 0.4316 - accuracy: 0.7849 - val_loss: 0.4288 - val_accuracy: 0.7849\n",
      "Epoch 8/50\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.42884 to 0.41820, saving model to ../model_weights/keras_weights_GRU.hdf5\n",
      "950255/950255 - 39s - loss: 0.4229 - accuracy: 0.7907 - val_loss: 0.4182 - val_accuracy: 0.7925\n",
      "Epoch 9/50\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.41820 to 0.41054, saving model to ../model_weights/keras_weights_GRU.hdf5\n",
      "950255/950255 - 39s - loss: 0.4152 - accuracy: 0.7955 - val_loss: 0.4105 - val_accuracy: 0.7969\n",
      "Epoch 10/50\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.41054 to 0.40379, saving model to ../model_weights/keras_weights_GRU.hdf5\n",
      "950255/950255 - 40s - loss: 0.4079 - accuracy: 0.8000 - val_loss: 0.4038 - val_accuracy: 0.8016\n",
      "Epoch 11/50\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.40379 to 0.39999, saving model to ../model_weights/keras_weights_GRU.hdf5\n",
      "950255/950255 - 39s - loss: 0.4018 - accuracy: 0.8038 - val_loss: 0.4000 - val_accuracy: 0.8048\n",
      "Epoch 12/50\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.39999 to 0.39282, saving model to ../model_weights/keras_weights_GRU.hdf5\n",
      "950255/950255 - 39s - loss: 0.3959 - accuracy: 0.8077 - val_loss: 0.3928 - val_accuracy: 0.8083\n",
      "Epoch 13/50\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.39282 to 0.38764, saving model to ../model_weights/keras_weights_GRU.hdf5\n",
      "950255/950255 - 39s - loss: 0.3912 - accuracy: 0.8107 - val_loss: 0.3876 - val_accuracy: 0.8122\n",
      "Epoch 14/50\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.38764 to 0.38760, saving model to ../model_weights/keras_weights_GRU.hdf5\n",
      "950255/950255 - 39s - loss: 0.3871 - accuracy: 0.8132 - val_loss: 0.3876 - val_accuracy: 0.8114\n",
      "Epoch 15/50\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.38760 to 0.38038, saving model to ../model_weights/keras_weights_GRU.hdf5\n",
      "950255/950255 - 39s - loss: 0.3836 - accuracy: 0.8157 - val_loss: 0.3804 - val_accuracy: 0.8163\n",
      "Epoch 16/50\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.38038 to 0.37794, saving model to ../model_weights/keras_weights_GRU.hdf5\n",
      "950255/950255 - 40s - loss: 0.3805 - accuracy: 0.8175 - val_loss: 0.3779 - val_accuracy: 0.8184\n",
      "Epoch 17/50\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.37794 to 0.37523, saving model to ../model_weights/keras_weights_GRU.hdf5\n",
      "950255/950255 - 39s - loss: 0.3777 - accuracy: 0.8193 - val_loss: 0.3752 - val_accuracy: 0.8202\n",
      "Epoch 18/50\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.37523 to 0.37339, saving model to ../model_weights/keras_weights_GRU.hdf5\n",
      "950255/950255 - 39s - loss: 0.3757 - accuracy: 0.8208 - val_loss: 0.3734 - val_accuracy: 0.8217\n",
      "Epoch 19/50\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.37339 to 0.37228, saving model to ../model_weights/keras_weights_GRU.hdf5\n",
      "950255/950255 - 39s - loss: 0.3734 - accuracy: 0.8221 - val_loss: 0.3723 - val_accuracy: 0.8224\n",
      "Epoch 20/50\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.37228 to 0.36895, saving model to ../model_weights/keras_weights_GRU.hdf5\n",
      "950255/950255 - 39s - loss: 0.3712 - accuracy: 0.8236 - val_loss: 0.3689 - val_accuracy: 0.8243\n",
      "Epoch 21/50\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.36895 to 0.36774, saving model to ../model_weights/keras_weights_GRU.hdf5\n",
      "950255/950255 - 39s - loss: 0.3694 - accuracy: 0.8248 - val_loss: 0.3677 - val_accuracy: 0.8251\n",
      "Epoch 22/50\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.36774 to 0.36589, saving model to ../model_weights/keras_weights_GRU.hdf5\n",
      "950255/950255 - 40s - loss: 0.3676 - accuracy: 0.8260 - val_loss: 0.3659 - val_accuracy: 0.8266\n",
      "Epoch 23/50\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.36589 to 0.36403, saving model to ../model_weights/keras_weights_GRU.hdf5\n",
      "950255/950255 - 39s - loss: 0.3663 - accuracy: 0.8266 - val_loss: 0.3640 - val_accuracy: 0.8274\n",
      "Epoch 24/50\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.36403 to 0.36286, saving model to ../model_weights/keras_weights_GRU.hdf5\n",
      "950255/950255 - 39s - loss: 0.3646 - accuracy: 0.8278 - val_loss: 0.3629 - val_accuracy: 0.8278\n",
      "Epoch 25/50\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.36286 to 0.36177, saving model to ../model_weights/keras_weights_GRU.hdf5\n",
      "950255/950255 - 39s - loss: 0.3632 - accuracy: 0.8288 - val_loss: 0.3618 - val_accuracy: 0.8284\n",
      "Epoch 26/50\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.36177 to 0.36132, saving model to ../model_weights/keras_weights_GRU.hdf5\n",
      "950255/950255 - 40s - loss: 0.3619 - accuracy: 0.8293 - val_loss: 0.3613 - val_accuracy: 0.8290\n",
      "Epoch 27/50\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.36132 to 0.35863, saving model to ../model_weights/keras_weights_GRU.hdf5\n",
      "950255/950255 - 39s - loss: 0.3606 - accuracy: 0.8302 - val_loss: 0.3586 - val_accuracy: 0.8311\n",
      "Epoch 28/50\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.35863 to 0.35734, saving model to ../model_weights/keras_weights_GRU.hdf5\n",
      "950255/950255 - 39s - loss: 0.3595 - accuracy: 0.8310 - val_loss: 0.3573 - val_accuracy: 0.8318\n",
      "Epoch 29/50\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.35734 to 0.35701, saving model to ../model_weights/keras_weights_GRU.hdf5\n",
      "950255/950255 - 39s - loss: 0.3585 - accuracy: 0.8313 - val_loss: 0.3570 - val_accuracy: 0.8318\n",
      "Epoch 30/50\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.35701\n",
      "950255/950255 - 39s - loss: 0.3572 - accuracy: 0.8322 - val_loss: 0.3571 - val_accuracy: 0.8318\n",
      "Epoch 31/50\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.35701 to 0.35486, saving model to ../model_weights/keras_weights_GRU.hdf5\n",
      "950255/950255 - 39s - loss: 0.3560 - accuracy: 0.8329 - val_loss: 0.3549 - val_accuracy: 0.8328\n",
      "Epoch 32/50\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.35486 to 0.35381, saving model to ../model_weights/keras_weights_GRU.hdf5\n",
      "950255/950255 - 39s - loss: 0.3554 - accuracy: 0.8333 - val_loss: 0.3538 - val_accuracy: 0.8332\n",
      "Epoch 33/50\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.35381 to 0.35265, saving model to ../model_weights/keras_weights_GRU.hdf5\n",
      "950255/950255 - 39s - loss: 0.3545 - accuracy: 0.8339 - val_loss: 0.3527 - val_accuracy: 0.8342\n",
      "Epoch 34/50\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.35265\n",
      "950255/950255 - 39s - loss: 0.3535 - accuracy: 0.8346 - val_loss: 0.3530 - val_accuracy: 0.8336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/50\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.35265 to 0.35088, saving model to ../model_weights/keras_weights_GRU.hdf5\n",
      "950255/950255 - 39s - loss: 0.3527 - accuracy: 0.8351 - val_loss: 0.3509 - val_accuracy: 0.8350\n",
      "Epoch 36/50\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.35088\n",
      "950255/950255 - 39s - loss: 0.3518 - accuracy: 0.8355 - val_loss: 0.3510 - val_accuracy: 0.8355\n",
      "Epoch 37/50\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.35088 to 0.34959, saving model to ../model_weights/keras_weights_GRU.hdf5\n",
      "950255/950255 - 39s - loss: 0.3512 - accuracy: 0.8357 - val_loss: 0.3496 - val_accuracy: 0.8359\n",
      "Epoch 38/50\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.34959 to 0.34907, saving model to ../model_weights/keras_weights_GRU.hdf5\n",
      "950255/950255 - 39s - loss: 0.3504 - accuracy: 0.8362 - val_loss: 0.3491 - val_accuracy: 0.8354\n",
      "Epoch 39/50\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.34907 to 0.34847, saving model to ../model_weights/keras_weights_GRU.hdf5\n",
      "950255/950255 - 39s - loss: 0.3496 - accuracy: 0.8366 - val_loss: 0.3485 - val_accuracy: 0.8356\n",
      "Epoch 40/50\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.34847\n",
      "950255/950255 - 39s - loss: 0.3490 - accuracy: 0.8371 - val_loss: 0.3495 - val_accuracy: 0.8355\n",
      "Epoch 41/50\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.34847 to 0.34741, saving model to ../model_weights/keras_weights_GRU.hdf5\n",
      "950255/950255 - 39s - loss: 0.3484 - accuracy: 0.8374 - val_loss: 0.3474 - val_accuracy: 0.8363\n",
      "Epoch 42/50\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.34741\n",
      "950255/950255 - 40s - loss: 0.3478 - accuracy: 0.8378 - val_loss: 0.3488 - val_accuracy: 0.8354\n",
      "Epoch 43/50\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.34741 to 0.34611, saving model to ../model_weights/keras_weights_GRU.hdf5\n",
      "950255/950255 - 39s - loss: 0.3471 - accuracy: 0.8380 - val_loss: 0.3461 - val_accuracy: 0.8372\n",
      "Epoch 44/50\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.34611 to 0.34533, saving model to ../model_weights/keras_weights_GRU.hdf5\n",
      "950255/950255 - 39s - loss: 0.3466 - accuracy: 0.8385 - val_loss: 0.3453 - val_accuracy: 0.8378\n",
      "Epoch 45/50\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.34533 to 0.34500, saving model to ../model_weights/keras_weights_GRU.hdf5\n",
      "950255/950255 - 39s - loss: 0.3462 - accuracy: 0.8389 - val_loss: 0.3450 - val_accuracy: 0.8379\n",
      "Epoch 46/50\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.34500 to 0.34427, saving model to ../model_weights/keras_weights_GRU.hdf5\n",
      "950255/950255 - 39s - loss: 0.3453 - accuracy: 0.8391 - val_loss: 0.3443 - val_accuracy: 0.8383\n",
      "Epoch 47/50\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.34427 to 0.34366, saving model to ../model_weights/keras_weights_GRU.hdf5\n",
      "950255/950255 - 39s - loss: 0.3448 - accuracy: 0.8395 - val_loss: 0.3437 - val_accuracy: 0.8383\n",
      "Epoch 48/50\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.34366\n",
      "950255/950255 - 39s - loss: 0.3443 - accuracy: 0.8399 - val_loss: 0.3460 - val_accuracy: 0.8379\n",
      "Epoch 49/50\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.34366 to 0.34264, saving model to ../model_weights/keras_weights_GRU.hdf5\n",
      "950255/950255 - 39s - loss: 0.3439 - accuracy: 0.8400 - val_loss: 0.3426 - val_accuracy: 0.8389\n",
      "Epoch 50/50\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.34264 to 0.34194, saving model to ../model_weights/keras_weights_GRU.hdf5\n",
      "950255/950255 - 39s - loss: 0.3433 - accuracy: 0.8404 - val_loss: 0.3419 - val_accuracy: 0.8389\n",
      "Model training time: 0:32:49.208322\n"
     ]
    }
   ],
   "source": [
    "training_start = datetime.datetime.now()\n",
    "model, callbacks_list = create_model(embedding_matrix, vocab_size, maxlen)\n",
    "history = model.fit(X_train, y_train, batch_size = 2048, epochs = 50, verbose=2, \n",
    "          validation_data=(X_val, y_val), callbacks = callbacks_list)\n",
    "training_stop = datetime.datetime.now()\n",
    "delta_training = training_stop - training_start\n",
    "print(\"Model training time:\", delta_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.835, Test: 0.834\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "before_train_evaluation = datetime.datetime.now()\n",
    "train_loss, train_acc = model.evaluate(X_train, y_train, verbose=0)\n",
    "after_train_evaluation = datetime.datetime.now()\n",
    "delta_train_evaluation = after_train_evaluation - before_train_evaluation\n",
    "print(\"Model evaluation time on training data\", delta_train_evaluation)\n",
    "\n",
    "before_test_evaluation = datetime.datetime.now()\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "after_test_evaluation = datetime.datetime.now()\n",
    "delta_test_evaluation = after_test_evaluation - before_test_evaluation\n",
    "print(\"Model evaluation time on testing data\", delta_test_evaluation)\n",
    "\n",
    "print('Training Loss: %.3f, Testing loss: %.3f' % (train_loss, test_loss) )\n",
    "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training history\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.sequential.Sequential at 0x7fe00224dc88>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saving the model and loading the saved model\n",
    "\n",
    "model.save('../saved_models/model_GRU_PCA_prune.h5')\n",
    "keras.models.load_model('../saved_models/model_GRU_PCA_prune.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning - Trim Insignificant Weights\n",
    "\n",
    "Magnitude-based weight pruning gradually zeroes out model weights during the training process to achieve model sparsity. Sparse models are easier to compress, and we can skip the zeroes during inference for latency improvements.\n",
    "\n",
    "This technique brings improvements via model compression. In the future, framework support for this technique will provide latency improvements. We've seen up to 6x improvements in model compression with minimal loss of accuracy.\n",
    "\n",
    "The technique is being evaluated in various speech applications, such as speech recognition and text-to-speech, and has been experimented on across various vision and translation models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import os\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline test accuracy: 0.8394188\n",
      "Saved baseline model to: /tmp/tmpgtmlehn4.h5\n"
     ]
    }
   ],
   "source": [
    "# Evaluate baseline model accuracy and saving it for later usage\n",
    "\n",
    "_, baseline_model_accuracy = model.evaluate(\n",
    "    X_test, y_test, verbose=0)\n",
    "\n",
    "print('Baseline test accuracy:', baseline_model_accuracy)\n",
    "\n",
    "_, keras_file = tempfile.mkstemp('.h5')\n",
    "model.save(keras_file, include_optimizer=False)\n",
    "print('Saved baseline model to:', keras_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tune pre-trained model with pruning  \n",
    "Define the model  \n",
    "You will apply pruning to the whole model and see this in the model summary.\n",
    "\n",
    "In this example, you start the model with 50% sparsity (50% zeros in weights) and end with 80% sparsity.\n",
    "\n",
    "In the comprehensive guide, you can see how to prune some layers for model accuracy improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "prune_low_magnitude_embeddin (None, 50, 16)            1250      \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_gru (Pru (None, 16)                3171      \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_dense (P (None, 8)                 266       \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_dense_1  (None, 1)                 19        \n",
      "=================================================================\n",
      "Total params: 4,706\n",
      "Trainable params: 2,401\n",
      "Non-trainable params: 2,305\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
    "\n",
    "# Compute end step to finish pruning after 2 epochs.\n",
    "batch_size = 2048\n",
    "epochs = 50\n",
    "validation_split = 0.1 # 10% of training set will be used for validation set. \n",
    "\n",
    "num_train = X_train.shape[0] * (1 - validation_split)\n",
    "end_step = np.ceil(num_train / batch_size).astype(np.int32) * epochs\n",
    "\n",
    "# Define model for pruning.\n",
    "pruning_params = {'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.30,\n",
    "                                                               final_sparsity=0.50,\n",
    "                                                               begin_step=0,\n",
    "                                                               end_step=end_step)\n",
    "}\n",
    "\n",
    "model_for_pruning = prune_low_magnitude(model, **pruning_params)\n",
    "optimizer = keras.optimizers.Adam(lr = 0.001, decay = 0.0001)\n",
    "# `prune_low_magnitude` requires a recompile.\n",
    "model_for_pruning.compile(optimizer=optimizer,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_for_pruning.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and evaluate the model against baseline  \n",
    "Fine tune with pruning for two epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 855229 samples, validate on 95026 samples\n",
      "Epoch 1/50\n",
      "  2048/855229 [..............................] - ETA: 28:50 - loss: 0.6096 - accuracy: 0.6729WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.218212). Check your callbacks.\n",
      "855229/855229 [==============================] - 42s 49us/sample - loss: 0.5036 - accuracy: 0.7378 - val_loss: 0.4596 - val_accuracy: 0.7640\n",
      "Epoch 2/50\n",
      "855229/855229 [==============================] - 37s 43us/sample - loss: 0.4401 - accuracy: 0.7778 - val_loss: 0.4267 - val_accuracy: 0.7854\n",
      "Epoch 3/50\n",
      "855229/855229 [==============================] - 37s 43us/sample - loss: 0.4162 - accuracy: 0.7939 - val_loss: 0.4099 - val_accuracy: 0.7972\n",
      "Epoch 4/50\n",
      "855229/855229 [==============================] - 37s 43us/sample - loss: 0.4030 - accuracy: 0.8030 - val_loss: 0.3992 - val_accuracy: 0.8042\n",
      "Epoch 5/50\n",
      "855229/855229 [==============================] - 37s 43us/sample - loss: 0.3951 - accuracy: 0.8080 - val_loss: 0.3950 - val_accuracy: 0.8076\n",
      "Epoch 6/50\n",
      "855229/855229 [==============================] - 37s 43us/sample - loss: 0.3894 - accuracy: 0.8117 - val_loss: 0.3888 - val_accuracy: 0.8099\n",
      "Epoch 7/50\n",
      "855229/855229 [==============================] - 37s 43us/sample - loss: 0.3853 - accuracy: 0.8145 - val_loss: 0.3851 - val_accuracy: 0.8139\n",
      "Epoch 8/50\n",
      "855229/855229 [==============================] - 37s 43us/sample - loss: 0.3820 - accuracy: 0.8166 - val_loss: 0.3828 - val_accuracy: 0.8141\n",
      "Epoch 9/50\n",
      "855229/855229 [==============================] - 37s 43us/sample - loss: 0.3791 - accuracy: 0.8182 - val_loss: 0.3794 - val_accuracy: 0.8177\n",
      "Epoch 10/50\n",
      "855229/855229 [==============================] - 37s 43us/sample - loss: 0.3768 - accuracy: 0.8197 - val_loss: 0.3773 - val_accuracy: 0.8188\n",
      "Epoch 11/50\n",
      "855229/855229 [==============================] - 37s 43us/sample - loss: 0.3749 - accuracy: 0.8208 - val_loss: 0.3756 - val_accuracy: 0.8190\n",
      "Epoch 12/50\n",
      "855229/855229 [==============================] - 37s 43us/sample - loss: 0.3730 - accuracy: 0.8217 - val_loss: 0.3740 - val_accuracy: 0.8209\n",
      "Epoch 13/50\n",
      "855229/855229 [==============================] - 37s 43us/sample - loss: 0.3717 - accuracy: 0.8228 - val_loss: 0.3725 - val_accuracy: 0.8223\n",
      "Epoch 14/50\n",
      "855229/855229 [==============================] - 37s 43us/sample - loss: 0.3702 - accuracy: 0.8233 - val_loss: 0.3712 - val_accuracy: 0.8225\n",
      "Epoch 15/50\n",
      "855229/855229 [==============================] - 37s 43us/sample - loss: 0.3688 - accuracy: 0.8244 - val_loss: 0.3699 - val_accuracy: 0.8243\n",
      "Epoch 16/50\n",
      "855229/855229 [==============================] - 37s 43us/sample - loss: 0.3674 - accuracy: 0.8252 - val_loss: 0.3689 - val_accuracy: 0.8244\n",
      "Epoch 17/50\n",
      "855229/855229 [==============================] - 37s 43us/sample - loss: 0.3674 - accuracy: 0.8254 - val_loss: 0.3684 - val_accuracy: 0.8245\n",
      "Epoch 18/50\n",
      "855229/855229 [==============================] - 37s 43us/sample - loss: 0.3661 - accuracy: 0.8260 - val_loss: 0.3675 - val_accuracy: 0.8255\n",
      "Epoch 19/50\n",
      "855229/855229 [==============================] - 37s 43us/sample - loss: 0.3650 - accuracy: 0.8268 - val_loss: 0.3671 - val_accuracy: 0.8259\n",
      "Epoch 20/50\n",
      "855229/855229 [==============================] - 37s 43us/sample - loss: 0.3643 - accuracy: 0.8272 - val_loss: 0.3679 - val_accuracy: 0.8256\n",
      "Epoch 21/50\n",
      "855229/855229 [==============================] - 37s 43us/sample - loss: 0.3634 - accuracy: 0.8279 - val_loss: 0.3649 - val_accuracy: 0.8266\n",
      "Epoch 22/50\n",
      "855229/855229 [==============================] - 37s 43us/sample - loss: 0.3626 - accuracy: 0.8281 - val_loss: 0.3645 - val_accuracy: 0.8271\n",
      "Epoch 23/50\n",
      "855229/855229 [==============================] - 37s 43us/sample - loss: 0.3620 - accuracy: 0.8285 - val_loss: 0.3634 - val_accuracy: 0.8278\n",
      "Epoch 24/50\n",
      "855229/855229 [==============================] - 37s 43us/sample - loss: 0.3613 - accuracy: 0.8289 - val_loss: 0.3629 - val_accuracy: 0.8283\n",
      "Epoch 25/50\n",
      "855229/855229 [==============================] - 36s 43us/sample - loss: 0.3608 - accuracy: 0.8293 - val_loss: 0.3626 - val_accuracy: 0.8286\n",
      "Epoch 26/50\n",
      "855229/855229 [==============================] - 37s 43us/sample - loss: 0.3601 - accuracy: 0.8299 - val_loss: 0.3619 - val_accuracy: 0.8295\n",
      "Epoch 27/50\n",
      "855229/855229 [==============================] - 37s 43us/sample - loss: 0.3596 - accuracy: 0.8302 - val_loss: 0.3614 - val_accuracy: 0.8291\n",
      "Epoch 28/50\n",
      "855229/855229 [==============================] - 37s 43us/sample - loss: 0.3591 - accuracy: 0.8307 - val_loss: 0.3611 - val_accuracy: 0.8298\n",
      "Epoch 29/50\n",
      "855229/855229 [==============================] - 37s 43us/sample - loss: 0.3586 - accuracy: 0.8309 - val_loss: 0.3608 - val_accuracy: 0.8300\n",
      "Epoch 30/50\n",
      "855229/855229 [==============================] - 37s 43us/sample - loss: 0.3579 - accuracy: 0.8313 - val_loss: 0.3612 - val_accuracy: 0.8298\n",
      "Epoch 31/50\n",
      "855229/855229 [==============================] - 37s 43us/sample - loss: 0.3576 - accuracy: 0.8316 - val_loss: 0.3598 - val_accuracy: 0.8304\n",
      "Epoch 32/50\n",
      "855229/855229 [==============================] - 37s 43us/sample - loss: 0.3571 - accuracy: 0.8318 - val_loss: 0.3590 - val_accuracy: 0.8308\n",
      "Epoch 33/50\n",
      "855229/855229 [==============================] - 37s 43us/sample - loss: 0.3568 - accuracy: 0.8321 - val_loss: 0.3590 - val_accuracy: 0.8302\n",
      "Epoch 34/50\n",
      "855229/855229 [==============================] - 37s 43us/sample - loss: 0.3562 - accuracy: 0.8324 - val_loss: 0.3597 - val_accuracy: 0.8306\n",
      "Epoch 35/50\n",
      "855229/855229 [==============================] - 37s 43us/sample - loss: 0.3559 - accuracy: 0.8325 - val_loss: 0.3585 - val_accuracy: 0.8315\n",
      "Epoch 36/50\n",
      "855229/855229 [==============================] - 37s 43us/sample - loss: 0.3556 - accuracy: 0.8329 - val_loss: 0.3582 - val_accuracy: 0.8317\n",
      "Epoch 37/50\n",
      "855229/855229 [==============================] - 37s 43us/sample - loss: 0.3552 - accuracy: 0.8330 - val_loss: 0.3575 - val_accuracy: 0.8321\n",
      "Epoch 38/50\n",
      "855229/855229 [==============================] - 37s 43us/sample - loss: 0.3549 - accuracy: 0.8333 - val_loss: 0.3572 - val_accuracy: 0.8321\n",
      "Epoch 39/50\n",
      "855229/855229 [==============================] - 37s 43us/sample - loss: 0.3548 - accuracy: 0.8334 - val_loss: 0.3573 - val_accuracy: 0.8315\n",
      "Epoch 40/50\n",
      "855229/855229 [==============================] - 37s 43us/sample - loss: 0.3546 - accuracy: 0.8333 - val_loss: 0.3587 - val_accuracy: 0.8313\n",
      "Epoch 41/50\n",
      "855229/855229 [==============================] - 37s 43us/sample - loss: 0.3544 - accuracy: 0.8336 - val_loss: 0.3568 - val_accuracy: 0.8322\n",
      "Epoch 42/50\n",
      "855229/855229 [==============================] - 37s 43us/sample - loss: 0.3539 - accuracy: 0.8337 - val_loss: 0.3567 - val_accuracy: 0.8326\n",
      "Epoch 43/50\n",
      "855229/855229 [==============================] - 37s 43us/sample - loss: 0.3535 - accuracy: 0.8340 - val_loss: 0.3559 - val_accuracy: 0.8329\n",
      "Epoch 44/50\n",
      "855229/855229 [==============================] - 37s 43us/sample - loss: 0.3534 - accuracy: 0.8340 - val_loss: 0.3556 - val_accuracy: 0.8332\n",
      "Epoch 45/50\n",
      "855229/855229 [==============================] - 37s 43us/sample - loss: 0.3531 - accuracy: 0.8342 - val_loss: 0.3558 - val_accuracy: 0.8329\n",
      "Epoch 46/50\n",
      "855229/855229 [==============================] - 37s 43us/sample - loss: 0.3528 - accuracy: 0.8343 - val_loss: 0.3554 - val_accuracy: 0.8333\n",
      "Epoch 47/50\n",
      "855229/855229 [==============================] - 37s 43us/sample - loss: 0.3526 - accuracy: 0.8344 - val_loss: 0.3551 - val_accuracy: 0.8331\n",
      "Epoch 48/50\n",
      "855229/855229 [==============================] - 37s 43us/sample - loss: 0.3524 - accuracy: 0.8345 - val_loss: 0.3554 - val_accuracy: 0.8332\n",
      "Epoch 49/50\n",
      "855229/855229 [==============================] - 37s 43us/sample - loss: 0.3522 - accuracy: 0.8347 - val_loss: 0.3549 - val_accuracy: 0.8332\n",
      "Epoch 50/50\n",
      "855229/855229 [==============================] - 37s 43us/sample - loss: 0.3520 - accuracy: 0.8348 - val_loss: 0.3545 - val_accuracy: 0.8336\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fdfda70f048>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logdir = tempfile.mkdtemp()\n",
    "\n",
    "callbacks = [\n",
    "  tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "  tfmot.sparsity.keras.PruningSummaries(log_dir=logdir),\n",
    "]\n",
    "  \n",
    "model_for_pruning.fit(X_train, y_train,\n",
    "                  batch_size=batch_size, epochs=epochs, validation_split=validation_split,\n",
    "                  callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline test accuracy: 0.8394188\n",
      "Pruned test accuracy: 0.83370584\n"
     ]
    }
   ],
   "source": [
    "_, model_for_pruning_accuracy = model_for_pruning.evaluate(\n",
    "   X_test, y_test, verbose=0)\n",
    "\n",
    "print('Baseline test accuracy:', baseline_model_accuracy) \n",
    "print('Pruned test accuracy:', model_for_pruning_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-8b4bea97dc69f387\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-8b4bea97dc69f387\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6007;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir={logdir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create 3x smaller models from pruning\n",
    "\n",
    "Both tfmot.sparsity.keras.strip_pruning and applying a standard compression algorithm (e.g. via gzip) are necessary to see the compression benefits of pruning.  \n",
    "\n",
    "First, create a compressible model for TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved pruned Keras model to: /tmp/tmpwdrwbcl7.h5\n"
     ]
    }
   ],
   "source": [
    "model_for_export = tfmot.sparsity.keras.strip_pruning(model_for_pruning)\n",
    "\n",
    "_, pruned_keras_file = tempfile.mkstemp('.h5')\n",
    "tf.keras.models.save_model(model_for_export, pruned_keras_file, include_optimizer=False)\n",
    "print('Saved pruned Keras model to:', pruned_keras_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, create a compressible model for TFLite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'kernel'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-0052d0b2e15c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFLiteConverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_keras_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_for_export\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_new_converter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpruned_tflite_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpruned_tflite_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtempfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkstemp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.tflite'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/research_data/anaconda3/lib/python3.7/site-packages/tensorflow_core/lite/python/lite.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[0;32m/research_data/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/convert_to_constants.py\u001b[0m in \u001b[0;36mconvert_variables_to_constants_v2\u001b[0;34m(func, lower_control_flow)\u001b[0m\n",
      "\u001b[0;32m/research_data/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/convert_to_constants.py\u001b[0m in \u001b[0;36m_get_control_flow_function_data\u001b[0;34m(node_defs, tensor_data)\u001b[0m\n",
      "\u001b[0;32m/research_data/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/convert_to_constants.py\u001b[0m in \u001b[0;36mget_resource_type\u001b[0;34m(node_name)\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'kernel'"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\n",
    "converter.experimental_new_converter = True\n",
    "pruned_tflite_model = converter.convert()\n",
    "\n",
    "_, pruned_tflite_file = tempfile.mkstemp('.tflite')\n",
    "\n",
    "with open(pruned_tflite_file, 'wb') as f:\n",
    "    f.write(pruned_tflite_model)\n",
    "\n",
    "print('Saved pruned TFLite model to:', pruned_tflite_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a helper function to actually compress the models via gzip and measure the zipped size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gzipped_model_size(file):\n",
    "  # Returns size of gzipped model, in bytes.\n",
    "    import os\n",
    "    import zipfile\n",
    "\n",
    "    _, zipped_file = tempfile.mkstemp('.zip')\n",
    "    with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
    "        f.write(file)\n",
    "\n",
    "    return os.path.getsize(zipped_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Size of gzipped baseline Keras model: %.2f bytes\" % (get_gzipped_model_size(keras_file)))\n",
    "print(\"Size of gzipped pruned Keras model: %.2f bytes\" % (get_gzipped_model_size(pruned_keras_file)))\n",
    "print(\"Size of gzipped pruned TFlite model: %.2f bytes\" % (get_gzipped_model_size(pruned_tflite_file)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict(text_x):\n",
    "# \tx = np.zeros((1, maxlen), dtype=np.int)\n",
    "# \toffset = max(maxlen - len(text_x), 0)\n",
    "# \tfor t, char in enumerate(text_x):\n",
    "# \t    if t >= maxlen:\n",
    "# \t        break\n",
    "# \t    x[0, t + offset] = char_indices[char]\n",
    "#     pred = model.predict(x)\n",
    "# \treturn pred[0][0]\n",
    "\n",
    "\n",
    "# Like predict, but you pass in an array of URLs, and it is all\n",
    "# vectorized in one step, making it more efficient\n",
    "def predicts(text_X):\n",
    "    X = np.zeros((len(text_X), maxlen), dtype=np.int)\n",
    "    for i in range(len(text_X)):\n",
    "        offset = max(maxlen - len(text_X[i]), 0)\n",
    "        for t, char in enumerate(text_X[i]):\n",
    "            if t >= maxlen:\n",
    "                break\n",
    "            X[i, t + offset] = char_indices[char]\n",
    "    preds = [pred[0] for pred in model.predict(X)]\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation\n",
    "# define a threshold value so that values below threshold will be classified as false_positive\n",
    "threshold = 0.5\n",
    "\n",
    "def evaluate_model(positives, negatives_train, negatives_dev, negatives_test, threshold):\n",
    "    false_negatives = 0.0\n",
    "    preds = predicts(positives)\n",
    "    for pred in preds:\n",
    "        if pred <= threshold:\n",
    "            false_negatives += 1\n",
    "    print(false_negatives / len(positives), \"false negatives for positives set.\")\n",
    "\n",
    "    false_positives_train = 0.0\n",
    "    preds = predicts(negatives_train)\n",
    "    for pred in preds:\n",
    "        if pred > threshold:\n",
    "            false_positives_train += 1\n",
    "\n",
    "    false_positives_dev = 0.0\n",
    "    preds = predicts(negatives_dev)\n",
    "    for pred in preds:\n",
    "        if pred > threshold:\n",
    "            false_positives_dev += 1\n",
    "\n",
    "    false_positives_test = 0.0\n",
    "    preds = predicts(negatives_test)\n",
    "    for pred in preds:\n",
    "        if pred > threshold:\n",
    "            false_positives_test += 1\n",
    "\n",
    "    print(false_positives_train / len(negatives_train), \"false positive rate for negative train.\")\n",
    "    print(false_positives_dev / len(negatives_dev), \"false positive rate for negative dev.\")\n",
    "    print(false_positives_test / len(negatives_test), \"false positive rate for negative test.\")\n",
    "\n",
    "evaluate_model(positives, negatives_train, negatives_dev, negatives_test, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting predictions on negative_dev set to find a suitable threshold value.\n",
    "\n",
    "# defining the false positive rate which we can change.\n",
    "fp_rate = 0.01\n",
    "\n",
    "print(\"Getting threshold for fp_rate\", fp_rate)\n",
    "preds = predicts(negatives_dev)\n",
    "preds.sort()\n",
    "fp_index = math.ceil((len(negatives_dev) * (1 - fp_rate)))\n",
    "threshold = preds[fp_index]\n",
    "\n",
    "print(\"Using threshold\", threshold) \n",
    "\n",
    "evaluate_model(positives, negatives_train, negatives_dev, negatives_test, threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bloom Filter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Adapted from https://www.geeksforgeeks.org/bloom-filters-introduction-and-python-implementation/ \n",
    "\n",
    "import math \n",
    "import mmh3 \n",
    "from bitarray import bitarray \n",
    "\n",
    "class BloomFilter(object): \n",
    "\n",
    "\t''' \n",
    "\tClass for Bloom filter, using murmur3 hash function \n",
    "\t'''\n",
    "\n",
    "\tdef __init__(self, items_count,fp_prob): \n",
    "\t\t''' \n",
    "\t\titems_count : int \n",
    "\t\t\tNumber of items expected to be stored in bloom filter \n",
    "\t\tfp_prob : float \n",
    "\t\t\tFalse Positive probability in decimal \n",
    "\t\t'''\n",
    "\t\t# False posible probability in decimal \n",
    "\t\tself.fp_prob = fp_prob \n",
    "\n",
    "\t\t# Size of bit array to use \n",
    "\t\tself.size = self.get_size(items_count,fp_prob) \n",
    "\n",
    "\t\t# number of hash functions to use \n",
    "\t\tself.hash_count = self.get_hash_count(self.size,items_count) \n",
    "\n",
    "\t\t# Bit array of given size \n",
    "\t\tself.bit_array = bitarray(self.size) \n",
    "        \n",
    "        # Return the size of bitarray in bytes\n",
    "        self.byte_size = self.bit_array.tobyte()\n",
    "\n",
    "\t\t# initialize all bits as 0 \n",
    "\t\tself.bit_array.setall(0) \n",
    "\n",
    "\tdef add(self, item): \n",
    "\t\t''' \n",
    "\t\tAdd an item in the filter \n",
    "\t\t'''\n",
    "\t\tdigests = [] \n",
    "\t\tfor i in range(self.hash_count): \n",
    "\n",
    "\t\t\t# create digest for given item. \n",
    "\t\t\t# i work as seed to mmh3.hash() function \n",
    "\t\t\t# With different seed, digest created is different \n",
    "\t\t\tdigest = mmh3.hash(item,i) % self.size \n",
    "\t\t\tdigests.append(digest) \n",
    "\n",
    "\t\t\t# set the bit True in bit_array \n",
    "\t\t\tself.bit_array[digest] = True\n",
    "\n",
    "\tdef check(self, item): \n",
    "\t\t''' \n",
    "\t\tCheck for existence of an item in filter \n",
    "\t\t'''\n",
    "\t\tfor i in range(self.hash_count): \n",
    "\t\t\tdigest = mmh3.hash(item,i) % self.size \n",
    "\t\t\tif self.bit_array[digest] == False: \n",
    "\n",
    "\t\t\t\t# if any of bit is False then,its not present \n",
    "\t\t\t\t# in filter \n",
    "\t\t\t\t# else there is probability that it exist \n",
    "\t\t\t\treturn False\n",
    "\t\treturn True\n",
    "\n",
    "\t@classmethod\n",
    "\tdef get_size(self,n,p): \n",
    "\t\t''' \n",
    "\t\tReturn the size of bit array(m) to used using \n",
    "\t\tfollowing formula \n",
    "\t\tm = -(n * lg(p)) / (lg(2)^2) \n",
    "\t\tn : int \n",
    "\t\t\tnumber of items expected to be stored in filter \n",
    "\t\tp : float \n",
    "\t\t\tFalse Positive probability in decimal \n",
    "\t\t'''\n",
    "\t\tm = -(n * math.log(p))/(math.log(2)**2) \n",
    "\t\treturn int(m)\n",
    "    \n",
    "\t@classmethod\n",
    "\tdef get_hash_count(self, m, n): \n",
    "\t\t''' \n",
    "\t\tReturn the hash function(k) to be used using \n",
    "\t\tfollowing formula \n",
    "\t\tk = (m/n) * lg(2) \n",
    "\n",
    "\t\tm : int \n",
    "\t\t\tsize of bit array \n",
    "\t\tn : int \n",
    "\t\t\tnumber of items expected to be stored in filter \n",
    "\t\t'''\n",
    "\t\tk = (m/n) * math.log(2) \n",
    "\t\treturn int(k) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Deep Bloom Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the threshold value\n",
    "fp_rate = 0.01\n",
    "print(\"Getting threshold for fp_rate\", fp_rate)\n",
    "preds = predicts(negatives_dev)\n",
    "preds.sort()\n",
    "fp_index = math.ceil((len(negatives_dev) * (1 - fp_rate/2)))\n",
    "threshold = preds[fp_index]\n",
    "print(\"The threhold value to use is:\", threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bloom_filter(data):\n",
    "    print(\"Creating bloom filter\")\n",
    "    false_negatives = []\n",
    "    # calling the predicts function \n",
    "    preds = predicts(data)\n",
    "    for i in range(len(data)):\n",
    "        if preds[i] <= threshold:\n",
    "            false_negatives.append(data[i])\n",
    "    print(\"Number of false negatives at bloom time\", len(false_negatives))\n",
    "    bloom_filter = BloomFilter(len(false_negatives), fp_rate / 2)\n",
    "    for fn in false_negatives:\n",
    "        bloom_filter.add(fn)\n",
    "    print(\"Created bloom filter\")\n",
    "    return bloom_filter\n",
    "\n",
    "bloom_filter = create_bloom_filter(positives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict the output from the machine learning model\n",
    "def predict(text_x):\n",
    "    x = np.zeros((1, maxlen), dtype=np.int)\n",
    "    offset = max(maxlen - len(text_x), 0)\n",
    "    for t, char in enumerate(text_x):\n",
    "        if t >= maxlen:\n",
    "            break\n",
    "        x[0, t + offset] = char_indices[char]\n",
    "    pred = model.predict(x)\n",
    "    return pred[0][0]\n",
    "\n",
    "\n",
    "def check_item(item):\n",
    "    if predict(item) > threshold:\n",
    "        return True\n",
    "    return bloom_filter.check(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Deep Bloom Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Bloom filter bits needed\", bloom_filter.size)\n",
    "print(\"Bloom fiter size in bytes\", bloom_filter.byte_size)\n",
    "print(\"Hash functions needed\", bloom_filter.hash_count)\n",
    "    \n",
    "false_positives = 0.0\n",
    "for neg in negatives_test:\n",
    "    if check_item(neg):\n",
    "        false_positives += 1\n",
    "print(\"Test false positive rate: \", str(false_positives / len(negatives_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
