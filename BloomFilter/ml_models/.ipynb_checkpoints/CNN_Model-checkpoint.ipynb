{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "import datetime\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from collections import Counter\n",
    "from keras.models import Sequential, load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data():\n",
    "    maxlen = 50\n",
    "    embedding_size = 50\n",
    "    \n",
    "    # importing the glove embeddings path \n",
    "    embeddings_path = '../data/glove.6B.50d-char.txt'\n",
    "    \n",
    "    # Indexing character vectors using glove word vectors\n",
    "    embedding_vectors = {}\n",
    "    with open(embeddings_path, 'r') as f:\n",
    "        for line in f:\n",
    "            line_split = line.strip().split(\" \")\n",
    "            vec = np.array(line_split[1:], dtype=float)\n",
    "            char = line_split[0]\n",
    "            embedding_vectors[char] = vec\n",
    "#     print('Found %s char vectors.' % len(embedding_vectors))\n",
    "    \n",
    "    # loading the dataset\n",
    "    with open('../data/dataset.json', 'r') as f:\n",
    "        dataset = json.load(f)\n",
    "        positives = dataset['positives']\n",
    "        negatives = dataset['negatives']\n",
    "    \n",
    "    # dividing the dataset to make small models\n",
    "    data_fraction = 1\n",
    "    positives = positives[:int(data_fraction * len(positives))]\n",
    "    negatives = negatives[:int(data_fraction * len(negatives))]\n",
    "    print(\"Total Dataset: \", len(positives+negatives))\n",
    "    \n",
    "    # Dividing the negatives dataset between train, dev and test\n",
    "    negatives_train = negatives[0: int(len(negatives) * .8)]\n",
    "    negatives_dev = negatives[int(len(negatives) * .8): int(len(negatives) * .9)]\n",
    "    negatives_test = negatives[int(len(negatives) * .9): ]\n",
    "    print(\"Split sizes:\")\n",
    "    print(len(positives), len(negatives_train), len(negatives_dev), len(negatives_test))\n",
    "    \n",
    "    # Shuffling the data\n",
    "    a = [(i, 0) for i in negatives_train]\n",
    "    b = [(i, 1) for i in positives]\n",
    "    combined = a + b\n",
    "    random.shuffle(combined)\n",
    "    shuffled = list(zip(*combined))\n",
    "    text_X = shuffled[0]\n",
    "    labels = shuffled[1]\n",
    "    \n",
    "    # tokenizing the input url's\n",
    "    tk = keras.preprocessing.text.Tokenizer(char_level=True)\n",
    "    tk.fit_on_texts(text_X)\n",
    "    \n",
    "    # List the vocabulary\n",
    "    word_index = tk.word_index\n",
    "    vocab_size = len(word_index) + 1\n",
    "#     print(vocab_size)\n",
    "#     print(word_index)\n",
    "    \n",
    "    # integer encode the documents\n",
    "    sequences = tk.texts_to_sequences(text_X)\n",
    "\n",
    "    # pad documents to a max length of 4 words\n",
    "    data = keras.preprocessing.sequence.pad_sequences(sequences, maxlen=maxlen) # by default the padding is post.\n",
    "    labels = np.asarray(labels)\n",
    "#     print('Shape of data tensor:', data.shape)\n",
    "#     print('Shape of label tensor:', labels.shape)\n",
    "    \n",
    "    # Dividing the dataset into train and test.\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # split the training data into a training set and a validation set\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "    \n",
    "    # create a weight matrix for words in training docs\n",
    "#     print('Preparing embedding matrix.')\n",
    "    embedding_matrix = np.zeros((vocab_size, 50))\n",
    "    for char, i in word_index.items():\n",
    "        embedding_vector = embedding_vectors.get(char)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "#     print(embedding_matrix.shape)\n",
    "    \n",
    "    # PCA Embedding dimension\n",
    "    pca_embedding_dim = 16\n",
    "    pca = PCA(n_components = pca_embedding_dim)\n",
    "    pca.fit(embedding_matrix[1:])\n",
    "    embedding_matrix_pca = np.array(pca.transform(embedding_matrix[1:]))\n",
    "    embedding_matrix_pca = np.insert(embedding_matrix_pca, 0, 0, axis=0)\n",
    "    print(\"PCA matrix created\")\n",
    "    \n",
    "    return (X_train, X_test, y_train, y_test, X_val, y_val, embedding_matrix_pca, maxlen, vocab_size, word_index,\n",
    "            positives, negatives_train, negatives_dev, negatives_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Less complicated CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(embedding_matrix, vocab_size, maxlen):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Embedding(vocab_size, 16, input_length=maxlen, weights=[embedding_matrix]))\n",
    "    model.add(keras.layers.Conv1D(128, 5, activation='relu'))\n",
    "    model.add(keras.layers.GlobalMaxPooling1D())\n",
    "    model.add(keras.layers.Dense(10, activation='relu'))\n",
    "    model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    optimizer = keras.optimizers.Adam(lr = 0.001, decay = 0.0001)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    earlyStopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=1, min_delta = 0.01)\n",
    "    file_path = '../model_weights/keras_weights_GRU_full_dataset_without_pca.hdf5'\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(filepath=file_path, verbose=1, save_best_only=True)\n",
    "    callbacks_list = [earlyStopping, checkpoint]\n",
    "    print(model.summary())\n",
    "    return model, callbacks_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complicated Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the model and the layers\n",
    "# print(\"Training the model:\")\n",
    "# embedding_layer = keras.layers.Embedding(vocab_size, 16, input_length=50, weights=[embedding_matrix_pca])\n",
    "# sequence_input = keras.layers.Input(shape=(50,), dtype='int32')\n",
    "# embedded_sequences = embedding_layer(sequence_input)\n",
    "# convs = []\n",
    "# filter_sizes = [3,4,5]\n",
    "# for filter_size in filter_sizes:\n",
    "#     l_conv = keras.layers.Conv1D(filters=256, kernel_size=filter_size)(embedded_sequences)\n",
    "#     l_batch = keras.layers.BatchNormalization()(l_conv)\n",
    "#     l_act = keras.layers.Activation('relu')(l_batch)\n",
    "#     l_drop = keras.layers.Dropout(0.3)(l_act)\n",
    "#     l_pool = keras.layers.GlobalMaxPooling1D()(l_drop)\n",
    "#     convs.append(l_pool)\n",
    "    \n",
    "# l_merge = keras.layers.concatenate(convs, axis=1)\n",
    "# x = keras.layers.Dense(128, activation='relu')(l_merge)\n",
    "# x = keras.layers.Dropout(0.4)(x)\n",
    "# preds = keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "# model = keras.Model(sequence_input, preds)\n",
    "# optimizer = keras.optimizers.Adam(lr = 0.001, decay = 0.0001)\n",
    "# model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# earlyStopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=1, min_delta = 0.01)\n",
    "# file_path = '../model_weights/keras_weights_GRU_full_dataset_without_pca.hdf5'\n",
    "# checkpoint = keras.callbacks.ModelCheckpoint(filepath=file_path, verbose=1, save_best_only=True)\n",
    "# callbacks_list = [earlyStopping, checkpoint]\n",
    "# print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Dataset:  2926705\n",
      "Split sizes:\n",
      "1491178 1148421 143553 143553\n",
      "PCA matrix created\n",
      "Data Preprocessing time: 0:01:17.856573\n"
     ]
    }
   ],
   "source": [
    "before_dataset = datetime.datetime.now()\n",
    "(X_train, X_test, y_train, y_test, X_val, y_val, embedding_matrix, maxlen, vocab_size, word_index,\n",
    "    positives, negatives_train, negatives_dev, negatives_test) = data()\n",
    "after_dataset = datetime.datetime.now()\n",
    "delta_dataset = after_dataset - before_dataset\n",
    "print(\"Data Preprocessing time:\", delta_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 50, 16)            624       \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 46, 128)           10368     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 12,293\n",
      "Trainable params: 12,293\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/40\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.32297, saving model to ../model_weights/keras_weights_GRU_full_dataset_without_pca.hdf5\n",
      "928/928 - 93s - loss: 0.4038 - accuracy: 0.8016 - val_loss: 0.3230 - val_accuracy: 0.8562\n",
      "Epoch 2/40\n"
     ]
    }
   ],
   "source": [
    "training_start = datetime.datetime.now()\n",
    "model, callbacks_list = create_model(embedding_matrix, vocab_size, maxlen)\n",
    "history = model.fit(X_train, y_train, batch_size = 2048, epochs = 40, verbose=2, \n",
    "          validation_data=(X_val, y_val), callbacks = callbacks_list)\n",
    "training_stop = datetime.datetime.now()\n",
    "delta_training = training_stop - training_start\n",
    "print(\"Model training time:\", delta_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "before_train_evaluation = datetime.datetime.now()\n",
    "train_loss, train_acc = model.evaluate(X_train, y_train, verbose=0)\n",
    "after_train_evaluation = datetime.datetime.now()\n",
    "delta_train_evaluation = after_train_evaluation - before_train_evaluation\n",
    "print(\"Model evaluation time on training data\", delta_train_evaluation)\n",
    "\n",
    "before_test_evaluation = datetime.datetime.now()\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "after_test_evaluation = datetime.datetime.now()\n",
    "delta_test_evaluation = after_test_evaluation - before_test_evaluation\n",
    "print(\"Model evaluation time on testing data\", delta_test_evaluation)\n",
    "\n",
    "print('Training Loss: %.3f, Testing loss: %.3f' % (train_loss, test_loss) )\n",
    "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model and loading the saved model\n",
    "\n",
    "model.save(\"../saved_models/full_model_CNN_with_pca_simpler.h5\")\n",
    "keras.models.load_model('../saved_models/full_model_CNN_with_pca_simpler.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Like predict, but you pass in an array of URLs, and it is all\n",
    "# vectorized in one step, making it more efficient\n",
    "def predicts(text_X):\n",
    "    X = np.zeros((len(text_X), maxlen), dtype=np.int)\n",
    "    for i in range(len(text_X)):\n",
    "        offset = max(maxlen - len(text_X[i]), 0)\n",
    "        for t, char in enumerate(text_X[i]):\n",
    "            if t >= maxlen:\n",
    "                break\n",
    "            X[i, t + offset] = word_index[char]\n",
    "    preds = [pred[0] for pred in model.predict(X)]\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation\n",
    "# define a threshold value so that values below threshold will be classified as false_positive\n",
    "threshold = 0.5\n",
    "maxlen = 50\n",
    "\n",
    "def evaluate_model(positives, negatives_train, negatives_dev, negatives_test, threshold):\n",
    "    false_negatives = 0.0\n",
    "    preds = predicts(positives)\n",
    "    for pred in preds:\n",
    "        if pred <= threshold:\n",
    "            false_negatives += 1\n",
    "    print(false_negatives / len(positives), \"false negatives for positives set.\")\n",
    "\n",
    "    false_positives_train = 0.0\n",
    "    preds = predicts(negatives_train)\n",
    "    for pred in preds:\n",
    "        if pred > threshold:\n",
    "            false_positives_train += 1\n",
    "\n",
    "    false_positives_dev = 0.0\n",
    "    preds = predicts(negatives_dev)\n",
    "    for pred in preds:\n",
    "        if pred > threshold:\n",
    "            false_positives_dev += 1\n",
    "\n",
    "    false_positives_test = 0.0\n",
    "    preds = predicts(negatives_test)\n",
    "    for pred in preds:\n",
    "        if pred > threshold:\n",
    "            false_positives_test += 1\n",
    "\n",
    "    print(false_positives_train / len(negatives_train), \"false positive rate for negatives train.\")\n",
    "    print(false_positives_dev / len(negatives_dev), \"false positive rate for negatives dev.\")\n",
    "    print(false_positives_test / len(negatives_test), \"false positive rate for negatives test.\")\n",
    "\n",
    "evaluate_model(positives, negatives_train, negatives_dev, negatives_test, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting predictions on negative_dev set to find a suitable threshold value.\n",
    "\n",
    "# defining the false positive rate which we can change.\n",
    "fp_rate = 0.01\n",
    "\n",
    "print(\"Getting threshold for fp_rate\", fp_rate)\n",
    "preds = predicts(negatives_dev)\n",
    "preds.sort()\n",
    "fp_index = math.ceil((len(negatives_dev) * (1 - fp_rate)))\n",
    "threshold = preds[fp_index]\n",
    "\n",
    "print(\"Using threshold\", threshold) \n",
    "\n",
    "evaluate_model(positives, negatives_train, negatives_dev, negatives_test, threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bloom Filter Implementation Using Murmur Hash Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Adapted from https://www.geeksforgeeks.org/bloom-filters-introduction-and-python-implementation/ \n",
    "\n",
    "import math \n",
    "import mmh3 \n",
    "from bitarray import bitarray \n",
    "\n",
    "class BloomFilter(object): \n",
    "    \n",
    "    ''' \n",
    "    Class for Bloom filter, using murmur3 hash function \n",
    "    '''\n",
    "    \n",
    "    def __init__(self, items_count,fp_prob): \n",
    "        ''' \n",
    "        items_count : int \n",
    "            Number of items expected to be stored in bloom filter \n",
    "        fp_prob : float \n",
    "            False Positive probability in decimal \n",
    "        '''\n",
    "        # False posible probability in decimal \n",
    "        self.fp_prob = fp_prob \n",
    "        \n",
    "        # Size of bit array to use \n",
    "        self.size = self.get_size(items_count,fp_prob) \n",
    "        \n",
    "        # number of hash functions to use \n",
    "        self.hash_count = self.get_hash_count(self.size,items_count) \n",
    "        \n",
    "        # Bit array of given size \n",
    "        self.bit_array = bitarray(self.size) \n",
    "                \n",
    "        # initialize all bits as 0 \n",
    "        self.bit_array.setall(0) \n",
    "\n",
    "    def add(self, item): \n",
    "        ''' \n",
    "        Add an item in the filter \n",
    "        '''\n",
    "        digests = [] \n",
    "        for i in range(self.hash_count): \n",
    "            \n",
    "            # create digest for given item. \n",
    "            # i work as seed to mmh3.hash() function \n",
    "            # With different seed, digest created is different \n",
    "            digest = mmh3.hash(item,i) % self.size \n",
    "            digests.append(digest) \n",
    "\n",
    "            # set the bit True in bit_array \n",
    "            self.bit_array[digest] = True\n",
    "\n",
    "    def check(self, item): \n",
    "        ''' \n",
    "        Check for existence of an item in filter \n",
    "        '''\n",
    "        for i in range(self.hash_count): \n",
    "            digest = mmh3.hash(item,i) % self.size \n",
    "            if self.bit_array[digest] == False: \n",
    "\n",
    "                # if any of bit is False then,its not present \n",
    "                # in filter \n",
    "                # else there is probability that it exist \n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    @classmethod\n",
    "    def get_size(self,n,p): \n",
    "        ''' \n",
    "        Return the size of bit array(m) to used using \n",
    "        following formula \n",
    "        m = -(n * lg(p)) / (lg(2)^2) \n",
    "        n : int \n",
    "            number of items expected to be stored in filter \n",
    "        p : float \n",
    "            False Positive probability in decimal \n",
    "        '''\n",
    "        m = -(n * math.log(p))/(math.log(2)**2) \n",
    "        return int(m) \n",
    "\n",
    "    @classmethod\n",
    "    def get_hash_count(self, m, n): \n",
    "        ''' \n",
    "        Return the hash function(k) to be used using \n",
    "        following formula \n",
    "        k = (m/n) * lg(2) \n",
    "        \n",
    "        m : int \n",
    "            size of bit array \n",
    "        n : int \n",
    "            number of items expected to be stored in filter \n",
    "        '''\n",
    "        k = (m/n) * math.log(2) \n",
    "        return int(k) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bloom_filter(data):\n",
    "    print(\"Creating bloom filter\")\n",
    "    false_negatives = []\n",
    "    # calling the predicts function \n",
    "    preds = predicts(data)\n",
    "    for i in range(len(data)):\n",
    "        if preds[i] <= threshold:\n",
    "            false_negatives.append(data[i])\n",
    "    print(\"Number of false negatives at bloom time\", len(false_negatives))\n",
    "    bloom_filter = BloomFilter(len(false_negatives), fp_rate / 2)\n",
    "    for fn in false_negatives:\n",
    "        bloom_filter.add(fn)\n",
    "    print(\"Created bloom filter\")\n",
    "    return bloom_filter\n",
    "\n",
    "bloom_filter = create_bloom_filter(positives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict the output from the machine learning model\n",
    "def predict(text_x):\n",
    "    x = np.zeros((1, maxlen), dtype=np.int)\n",
    "    offset = max(maxlen - len(text_x), 0)\n",
    "    for t, char in enumerate(text_x):\n",
    "        if t >= maxlen:\n",
    "            break\n",
    "        x[0, t + offset] = word_index[char]\n",
    "    pred = model.predict(x)\n",
    "    return pred[0][0]\n",
    "\n",
    "def check_item(item):\n",
    "    if predict(item) > threshold:\n",
    "        return True\n",
    "    return bloom_filter.check(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Bloom filter bits needed\", bloom_filter.size)\n",
    "print(\"Bloom filter size in bytes\", (bloom_filter.size)/8)\n",
    "print(\"Number of hash\", bloom_filter.hash_count)\n",
    "    \n",
    "false_positives = 0.0\n",
    "for neg in negatives_test:\n",
    "    if check_item(neg):\n",
    "        false_positives += 1\n",
    "print(\"Test false positive rate: \", str(false_positives / len(negatives_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
