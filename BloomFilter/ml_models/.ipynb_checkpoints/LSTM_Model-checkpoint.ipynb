{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "# Importing required pythin libraries\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "import datetime\n",
    "from collections import Counter\n",
    "\n",
    "# Importing required libraries from Scikit-learn\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Importing required libraries from keras+tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "tf.compat.v1.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data():\n",
    "    maxlen = 50\n",
    "    embedding_size = 50\n",
    "    \n",
    "    # importing the glove embeddings path \n",
    "    embeddings_path = '../data/glove.6B.50d-char.txt'\n",
    "    \n",
    "    # Indexing character vectors using glove word vectors\n",
    "    embedding_vectors = {}\n",
    "    with open(embeddings_path, 'r') as f:\n",
    "        for line in f:\n",
    "            line_split = line.strip().split(\" \")\n",
    "            vec = np.array(line_split[1:], dtype=float)\n",
    "            char = line_split[0]\n",
    "            embedding_vectors[char] = vec\n",
    "    print('Found %s char vectors.' % len(embedding_vectors))\n",
    "    \n",
    "    # loading the dataset\n",
    "    with open('../data/dataset.json', 'r') as f:\n",
    "        dataset = json.load(f)\n",
    "        positives = dataset['positives']\n",
    "        negatives = dataset['negatives']\n",
    "    \n",
    "    # dividing the dataset to make small models\n",
    "    data_fraction = 1\n",
    "    positives = positives[:int(data_fraction * len(positives))]\n",
    "    negatives = negatives[:int(data_fraction * len(negatives))]\n",
    "    \n",
    "    # Dividing the negatives dataset between train, dev and test\n",
    "    negatives_train = negatives[0: int(len(negatives) * .8)]\n",
    "    negatives_dev = negatives[int(len(negatives) * .8): int(len(negatives) * .9)]\n",
    "    negatives_test = negatives[int(len(negatives) * .9): ]\n",
    "    print(\"Split sizes:\")\n",
    "    print(len(positives), len(negatives_train), len(negatives_dev), len(negatives_test))\n",
    "    \n",
    "    # Shuffling the data\n",
    "    a = [(i, 0) for i in negatives_train]\n",
    "    b = [(i, 1) for i in positives]\n",
    "    combined = a + b\n",
    "    random.shuffle(combined)\n",
    "    shuffled = list(zip(*combined))\n",
    "    text_X = shuffled[0]\n",
    "    labels = shuffled[1]\n",
    "    \n",
    "    # tokenizing the input url's\n",
    "    tk = keras.preprocessing.text.Tokenizer(char_level=True)\n",
    "    tk.fit_on_texts(text_X)\n",
    "    \n",
    "    # List the vocabulary\n",
    "    word_index = tk.word_index\n",
    "    vocab_size = len(word_index) + 1\n",
    "#     print(vocab_size)\n",
    "#     print(word_index)\n",
    "    \n",
    "    # integer encode the documents\n",
    "    sequences = tk.texts_to_sequences(text_X)\n",
    "#     print(text_X[0])\n",
    "#     print(sequences[0])\n",
    "    \n",
    "    # pad documents to a max length of 4 words\n",
    "    data = keras.preprocessing.sequence.pad_sequences(sequences, maxlen=maxlen) # by default the padding is post.\n",
    "    labels = np.asarray(labels)\n",
    "#     print('Shape of data tensor:', data.shape)\n",
    "#     print('Shape of label tensor:', labels.shape)\n",
    "    \n",
    "    # Dividing the dataset into train and test.\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # split the training data into a training set and a validation set\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "    \n",
    "    # create a weight matrix for words in training docs\n",
    "    print('Preparing embedding matrix.')\n",
    "    embedding_matrix = np.zeros((vocab_size, 50))\n",
    "    for char, i in word_index.items():\n",
    "        embedding_vector = embedding_vectors.get(char)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    print(embedding_matrix.shape)\n",
    "    \n",
    "        # PCA Embedding dimension\n",
    "    pca_embedding_dim = 16\n",
    "    pca = PCA(n_components = pca_embedding_dim)\n",
    "    pca.fit(embedding_matrix[1:])\n",
    "    embedding_matrix_pca = np.array(pca.transform(embedding_matrix[1:]))\n",
    "    embedding_matrix_pca = np.insert(embedding_matrix_pca, 0, 0, axis=0)\n",
    "    print(\"PCA matrix created\")\n",
    "    \n",
    "    return (X_train, X_test, y_train, y_test, X_val, y_val, embedding_matrix_pca, maxlen, vocab_size, word_index,\n",
    "            positives, negatives_train, negatives_dev, negatives_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining LSTM Model\n",
    "\n",
    "The model is defined using one embedding layer, a combination of one or more LSTM layers and joinining them with \n",
    "Dense layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(embedding_matrix, vocab_size):\n",
    "    \n",
    "    model = keras.models.Sequential([\n",
    "        keras.layers.Embedding(vocab_size, 16, input_length=50, weights=[embedding_matrix]),\n",
    "        keras.layers.LSTM(16, return_sequences=False),\n",
    "        keras.layers.Dense(8, activation='relu'),\n",
    "        keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    optimizer = keras.optimizers.Adam(lr = 0.001, decay = 0.0001)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    earlyStopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=1, min_delta = 0.001)\n",
    "    file_path = '../model_weights/keras_weights_LSTM_full_dataset_without_pca.hdf5'\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(filepath=file_path, verbose=1, save_best_only=True)\n",
    "    callbacks_list = [earlyStopping, checkpoint]\n",
    "    print(model.summary())\n",
    "    return model, callbacks_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 68 char vectors.\n",
      "Split sizes:\n",
      "1491178 1148421 143553 143553\n",
      "Preparing embedding matrix.\n",
      "(39, 50)\n",
      "PCA matrix created\n",
      "Data Preprocessing time: 0:01:24.459911\n"
     ]
    }
   ],
   "source": [
    "before_dataset = datetime.datetime.now()\n",
    "(X_train, X_test, y_train, y_test, X_val, y_val, embedding_matrix, maxlen, vocab_size, word_index,\n",
    "    positives, negatives_train, negatives_dev, negatives_test) = data()\n",
    "after_dataset = datetime.datetime.now()\n",
    "delta_dataset = after_dataset - before_dataset\n",
    "print(\"Data Preprocessing time:\", delta_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 50, 16)            624       \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 16)                2112      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 2,881\n",
      "Trainable params: 2,881\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/40\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.51189, saving model to ../model_weights/keras_weights_LSTM_full_dataset_without_pca.hdf5\n",
      "928/928 - 114s - loss: 0.5491 - accuracy: 0.6992 - val_loss: 0.5119 - val_accuracy: 0.7266\n",
      "Epoch 2/40\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.51189 to 0.47628, saving model to ../model_weights/keras_weights_LSTM_full_dataset_without_pca.hdf5\n",
      "928/928 - 113s - loss: 0.4953 - accuracy: 0.7373 - val_loss: 0.4763 - val_accuracy: 0.7507\n",
      "Epoch 3/40\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.47628 to 0.45475, saving model to ../model_weights/keras_weights_LSTM_full_dataset_without_pca.hdf5\n",
      "928/928 - 113s - loss: 0.4660 - accuracy: 0.7594 - val_loss: 0.4548 - val_accuracy: 0.7668\n",
      "Epoch 4/40\n"
     ]
    }
   ],
   "source": [
    "training_start = datetime.datetime.now()\n",
    "model, callbacks_list = create_model(embedding_matrix, vocab_size)\n",
    "history = model.fit(X_train, y_train, batch_size = 2048, epochs = 40, verbose=2, \n",
    "          validation_data=(X_val, y_val), callbacks = callbacks_list)\n",
    "training_stop = datetime.datetime.now()\n",
    "delta_training = training_stop - training_start\n",
    "print(\"Model training time:\", delta_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model evaluation time on training data 0:06:12.083656\n",
      "Model evaluation time on testing data 0:01:38.922634\n",
      "Training Loss: 0.321, Testing loss: 0.322\n",
      "Train: 0.854, Test: 0.853\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "before_train_evaluation = datetime.datetime.now()\n",
    "train_loss, train_acc = model.evaluate(X_train, y_train, verbose=0)\n",
    "after_train_evaluation = datetime.datetime.now()\n",
    "delta_train_evaluation = after_train_evaluation - before_train_evaluation\n",
    "print(\"Model evaluation time on training data\", delta_train_evaluation)\n",
    "\n",
    "before_test_evaluation = datetime.datetime.now()\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "after_test_evaluation = datetime.datetime.now()\n",
    "delta_test_evaluation = after_test_evaluation - before_test_evaluation\n",
    "print(\"Model evaluation time on testing data\", delta_test_evaluation)\n",
    "\n",
    "print('Training Loss: %.3f, Testing loss: %.3f' % (train_loss, test_loss) )\n",
    "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.sequential.Sequential at 0x7f3dbb16a470>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saving the model and loading the saved model\n",
    "\n",
    "model.save('../saved_models/full_model_LSTM_with_pca.h5')\n",
    "keras.models.load_model('../saved_models/full_model_LSTM_with_pca.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Like predict, but you pass in an array of URLs, and it is all\n",
    "# vectorized in one step, making it more efficient\n",
    "def predicts(text_X):\n",
    "    X = np.zeros((len(text_X), maxlen), dtype=np.int)\n",
    "    for i in range(len(text_X)):\n",
    "        offset = max(maxlen - len(text_X[i]), 0)\n",
    "        for t, char in enumerate(text_X[i]):\n",
    "            if t >= maxlen:\n",
    "                break\n",
    "            X[i, t + offset] = word_index[char]\n",
    "    preds = [pred[0] for pred in model.predict(X)]\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1661659439718129 false negatives for positives set.\n",
      "0.11967214113987815 false positive rate for train.\n",
      "0.11884112488070608 false positive rate for dev.\n",
      "0.12064533656558901 false positive rate for test.\n"
     ]
    }
   ],
   "source": [
    "# Model Evaluation\n",
    "# define a threshold value so that values below threshold will be classified as false_positive\n",
    "threshold = 0.5\n",
    "maxlen = 50\n",
    "\n",
    "def evaluate_model(positives, negatives_train, negatives_dev, negatives_test, threshold):\n",
    "    false_negatives = 0.0\n",
    "    preds = predicts(positives)\n",
    "    for pred in preds:\n",
    "        if pred <= threshold:\n",
    "            false_negatives += 1\n",
    "    print(false_negatives / len(positives), \"false negatives for positives set.\")\n",
    "\n",
    "    false_positives_train = 0.0\n",
    "    preds = predicts(negatives_train)\n",
    "    for pred in preds:\n",
    "        if pred > threshold:\n",
    "            false_positives_train += 1\n",
    "\n",
    "    false_positives_dev = 0.0\n",
    "    preds = predicts(negatives_dev)\n",
    "    for pred in preds:\n",
    "        if pred > threshold:\n",
    "            false_positives_dev += 1\n",
    "\n",
    "    false_positives_test = 0.0\n",
    "    preds = predicts(negatives_test)\n",
    "    for pred in preds:\n",
    "        if pred > threshold:\n",
    "            false_positives_test += 1\n",
    "\n",
    "    print(false_positives_train / len(negatives_train), \"false positive rate for train.\")\n",
    "    print(false_positives_dev / len(negatives_dev), \"false positive rate for dev.\")\n",
    "    print(false_positives_test / len(negatives_test), \"false positive rate for test.\")\n",
    "\n",
    "evaluate_model(positives, negatives_train, negatives_dev, negatives_test, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting threshold for fp_rate 0.01\n",
      "Using threshold 0.9372022\n",
      "0.4281681999063827 false negatives for positives set.\n",
      "0.009815215848543347 false positive rate for train.\n",
      "0.009989341915529457 false positive rate for dev.\n",
      "0.009829122345057227 false positive rate for test.\n"
     ]
    }
   ],
   "source": [
    "# Getting predictions on negative_dev set to find a suitable threshold value.\n",
    "\n",
    "# defining the false positive rate which we can change.\n",
    "fp_rate = 0.01\n",
    "\n",
    "print(\"Getting threshold for fp_rate\", fp_rate)\n",
    "preds = predicts(negatives_dev)\n",
    "preds.sort()\n",
    "fp_index = math.ceil((len(negatives_dev) * (1 - fp_rate)))\n",
    "threshold = preds[fp_index]\n",
    "\n",
    "print(\"Using threshold\", threshold) \n",
    "\n",
    "evaluate_model(positives, negatives_train, negatives_dev, negatives_test, threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Bloom Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Adapted from https://www.geeksforgeeks.org/bloom-filters-introduction-and-python-implementation/ \n",
    "\n",
    "import math \n",
    "import mmh3 \n",
    "from bitarray import bitarray \n",
    "\n",
    "class BloomFilter(object): \n",
    "\n",
    "\t''' \n",
    "\tClass for Bloom filter, using murmur3 hash function \n",
    "\t'''\n",
    "\n",
    "\tdef __init__(self, items_count,fp_prob): \n",
    "\t\t''' \n",
    "\t\titems_count : int \n",
    "\t\t\tNumber of items expected to be stored in bloom filter \n",
    "\t\tfp_prob : float \n",
    "\t\t\tFalse Positive probability in decimal \n",
    "\t\t'''\n",
    "\t\t# False posible probability in decimal \n",
    "\t\tself.fp_prob = fp_prob \n",
    "\n",
    "\t\t# Size of bit array to use \n",
    "\t\tself.size = self.get_size(items_count,fp_prob) \n",
    "\n",
    "\t\t# number of hash functions to use \n",
    "\t\tself.hash_count = self.get_hash_count(self.size,items_count) \n",
    "\n",
    "\t\t# Bit array of given size \n",
    "\t\tself.bit_array = bitarray(self.size) \n",
    "\n",
    "\t\t# initialize all bits as 0 \n",
    "\t\tself.bit_array.setall(0) \n",
    "\n",
    "\tdef add(self, item): \n",
    "\t\t''' \n",
    "\t\tAdd an item in the filter \n",
    "\t\t'''\n",
    "\t\tdigests = [] \n",
    "\t\tfor i in range(self.hash_count): \n",
    "\n",
    "\t\t\t# create digest for given item. \n",
    "\t\t\t# i work as seed to mmh3.hash() function \n",
    "\t\t\t# With different seed, digest created is different \n",
    "\t\t\tdigest = mmh3.hash(item,i) % self.size \n",
    "\t\t\tdigests.append(digest) \n",
    "\n",
    "\t\t\t# set the bit True in bit_array \n",
    "\t\t\tself.bit_array[digest] = True\n",
    "\n",
    "\tdef check(self, item): \n",
    "\t\t''' \n",
    "\t\tCheck for existence of an item in filter \n",
    "\t\t'''\n",
    "\t\tfor i in range(self.hash_count): \n",
    "\t\t\tdigest = mmh3.hash(item,i) % self.size \n",
    "\t\t\tif self.bit_array[digest] == False: \n",
    "\n",
    "\t\t\t\t# if any of bit is False then,its not present \n",
    "\t\t\t\t# in filter \n",
    "\t\t\t\t# else there is probability that it exist \n",
    "\t\t\t\treturn False\n",
    "\t\treturn True\n",
    "\n",
    "\t@classmethod\n",
    "\tdef get_size(self,n,p): \n",
    "\t\t''' \n",
    "\t\tReturn the size of bit array(m) to used using \n",
    "\t\tfollowing formula \n",
    "\t\tm = -(n * lg(p)) / (lg(2)^2) \n",
    "\t\tn : int \n",
    "\t\t\tnumber of items expected to be stored in filter \n",
    "\t\tp : float \n",
    "\t\t\tFalse Positive probability in decimal \n",
    "\t\t'''\n",
    "\t\tm = -(n * math.log(p))/(math.log(2)**2) \n",
    "\t\treturn int(m) \n",
    "\n",
    "\t@classmethod\n",
    "\tdef get_hash_count(self, m, n): \n",
    "\t\t''' \n",
    "\t\tReturn the hash function(k) to be used using \n",
    "\t\tfollowing formula \n",
    "\t\tk = (m/n) * lg(2) \n",
    "\n",
    "\t\tm : int \n",
    "\t\t\tsize of bit array \n",
    "\t\tn : int \n",
    "\t\t\tnumber of items expected to be stored in filter \n",
    "\t\t'''\n",
    "\t\tk = (m/n) * math.log(2) \n",
    "\t\treturn int(k) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating bloom filter\n",
      "Number of false negatives at bloom time 638475\n",
      "Created bloom filter\n"
     ]
    }
   ],
   "source": [
    "def create_bloom_filter(data):\n",
    "    print(\"Creating bloom filter\")\n",
    "    false_negatives = []\n",
    "    # calling the predicts function \n",
    "    preds = predicts(data)\n",
    "    for i in range(len(data)):\n",
    "        if preds[i] <= threshold:\n",
    "            false_negatives.append(data[i])\n",
    "    print(\"Number of false negatives at bloom time\", len(false_negatives))\n",
    "    bloom_filter = BloomFilter(len(false_negatives), fp_rate / 2)\n",
    "    for fn in false_negatives:\n",
    "        bloom_filter.add(fn)\n",
    "    print(\"Created bloom filter\")\n",
    "    return bloom_filter\n",
    "\n",
    "bloom_filter = create_bloom_filter(positives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict the output from the machine learning model\n",
    "def predict(text_x):\n",
    "    x = np.zeros((1, maxlen), dtype=np.int)\n",
    "    offset = max(maxlen - len(text_x), 0)\n",
    "    for t, char in enumerate(text_x):\n",
    "        if t >= maxlen:\n",
    "            break\n",
    "        x[0, t + offset] = word_index[char]\n",
    "    pred = model.predict(x)\n",
    "    return pred[0][0]\n",
    "\n",
    "def check_item(item):\n",
    "    if predict(item) > threshold:\n",
    "        return True\n",
    "    return bloom_filter.check(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bloom filter bits needed 7040944\n",
      "Bloom filter size in bytes 880118.0\n",
      "Number of hash 7\n",
      "Test false positive rate:  0.014761098688289342\n"
     ]
    }
   ],
   "source": [
    "print(\"Bloom filter bits needed\", bloom_filter.size)\n",
    "print(\"Bloom filter size in bytes\", (bloom_filter.size)/8)\n",
    "print(\"Number of hash\", bloom_filter.hash_count)\n",
    "    \n",
    "false_positives = 0.0\n",
    "for neg in negatives_test:\n",
    "    if check_item(neg):\n",
    "        false_positives += 1\n",
    "print(\"Test false positive rate: \", str(false_positives / len(negatives_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
