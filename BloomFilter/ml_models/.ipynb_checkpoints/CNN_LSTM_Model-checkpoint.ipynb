{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 6.2 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "# Python imports\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "import datetime\n",
    "from collections import Counter\n",
    "\n",
    "# Scikit-learn imports \n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Tensorflow and Keras imports\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.keras import backend as K\n",
    "\n",
    "K.clear_session()\n",
    "tf.compat.v1.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data():\n",
    "    maxlen = 50\n",
    "    embedding_size = 50\n",
    "    \n",
    "    # importing the glove embeddings path \n",
    "    embeddings_path = '../data/glove.6B.50d-char.txt'\n",
    "    \n",
    "    # Indexing character vectors using glove word vectors\n",
    "    embedding_vectors = {}\n",
    "    with open(embeddings_path, 'r') as f:\n",
    "        for line in f:\n",
    "            line_split = line.strip().split(\" \")\n",
    "            vec = np.array(line_split[1:], dtype=float)\n",
    "            char = line_split[0]\n",
    "            embedding_vectors[char] = vec\n",
    "#     print('Found %s char vectors.' % len(embedding_vectors))\n",
    "    \n",
    "    # loading the dataset\n",
    "    with open('../data/dataset.json', 'r') as f:\n",
    "        dataset = json.load(f)\n",
    "        positives = dataset['positives']\n",
    "        negatives = dataset['negatives']\n",
    "    \n",
    "    # dividing the dataset to make small models\n",
    "    data_fraction = 1\n",
    "    positives = positives[:int(data_fraction * len(positives))]\n",
    "    negatives = negatives[:int(data_fraction * len(negatives))]\n",
    "    \n",
    "    # Dividing the negatives dataset between train, dev and test\n",
    "    negatives_train = negatives[0: int(len(negatives) * .8)]\n",
    "    negatives_dev = negatives[int(len(negatives) * .8): int(len(negatives) * .9)]\n",
    "    negatives_test = negatives[int(len(negatives) * .9): ]\n",
    "    print(\"Split sizes:\")\n",
    "    print(len(positives), len(negatives_train), len(negatives_dev), len(negatives_test))\n",
    "    \n",
    "    # Shuffling the data\n",
    "    a = [(i, 0) for i in negatives_train]\n",
    "    b = [(i, 1) for i in positives]\n",
    "    combined = a + b\n",
    "    random.shuffle(combined)\n",
    "    shuffled = list(zip(*combined))\n",
    "    text_X = shuffled[0]\n",
    "    labels = shuffled[1]\n",
    "    \n",
    "    # tokenizing the input url's\n",
    "    tk = keras.preprocessing.text.Tokenizer(char_level=True)\n",
    "    tk.fit_on_texts(text_X)\n",
    "    \n",
    "    # List the vocabulary\n",
    "    word_index = tk.word_index\n",
    "    vocab_size = len(word_index) + 1\n",
    "#     print(vocab_size)\n",
    "#     print(word_index)\n",
    "    \n",
    "    # integer encode the documents\n",
    "    sequences = tk.texts_to_sequences(text_X)\n",
    "#     print(text_X[0])\n",
    "#     print(sequences[0])\n",
    "    \n",
    "    # pad documents to a max length of 4 words\n",
    "    data = keras.preprocessing.sequence.pad_sequences(sequences, maxlen=maxlen) # by default the padding is post.\n",
    "    labels = np.asarray(labels)\n",
    "#     print('Shape of data tensor:', data.shape)\n",
    "#     print('Shape of label tensor:', labels.shape)\n",
    "    \n",
    "    # Dividing the dataset into train and test.\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # split the training data into a training set and a validation set\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "    \n",
    "    # create a weight matrix for words in training docs\n",
    "    print('Preparing embedding matrix.')\n",
    "    embedding_matrix = np.zeros((vocab_size, 50))\n",
    "    for char, i in word_index.items():\n",
    "        embedding_vector = embedding_vectors.get(char)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    print(embedding_matrix.shape)\n",
    "    \n",
    "    # PCA Embedding dimension\n",
    "    pca_embedding_dim = 16\n",
    "    pca = PCA(n_components = pca_embedding_dim)\n",
    "    pca.fit(embedding_matrix[1:])\n",
    "    embedding_matrix_pca = np.array(pca.transform(embedding_matrix[1:]))\n",
    "    embedding_matrix_pca = np.insert(embedding_matrix_pca, 0, 0, axis=0)\n",
    "    print(\"PCA matrix created\")\n",
    "    \n",
    "    return (X_train, X_test, y_train, y_test, X_val, y_val, embedding_matrix_pca, maxlen, vocab_size, word_index,\n",
    "            positives, negatives_train, negatives_dev, negatives_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "def create_model(embedding_matrix, maxlen, vocab_size):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Embedding(vocab_size, 16, input_length=maxlen, weights=[embedding_matrix]))\n",
    "    model.add(keras.layers.Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "    model.add(keras.layers.MaxPooling1D(pool_size=2))\n",
    "    model.add(keras.layers.LSTM(100))\n",
    "    model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split sizes:\n",
      "1491178 1148421 143553 143553\n",
      "Preparing embedding matrix.\n",
      "(39, 50)\n",
      "PCA matrix created\n",
      "Data Preprocessing time: 0:01:08.084457\n"
     ]
    }
   ],
   "source": [
    "before_dataset = datetime.datetime.now()\n",
    "(X_train, X_test, y_train, y_test, X_val, y_val, embedding_matrix, maxlen, vocab_size, word_index,\n",
    "            positives, negatives_train, negatives_dev, negatives_test) = data()\n",
    "after_dataset = datetime.datetime.now()\n",
    "delta_dataset = after_dataset - before_dataset\n",
    "print(\"Data Preprocessing time:\", delta_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 50, 16)            624       \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 50, 32)            1568      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 25, 32)            0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 55,493\n",
      "Trainable params: 55,493\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/40\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.38874, saving model to keras_weights_GRU.hdf5\n",
      "928/928 - 145s - loss: 0.4486 - accuracy: 0.7689 - val_loss: 0.3887 - val_accuracy: 0.8124\n",
      "Epoch 2/40\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.38874 to 0.35930, saving model to keras_weights_GRU.hdf5\n",
      "928/928 - 144s - loss: 0.3695 - accuracy: 0.8231 - val_loss: 0.3593 - val_accuracy: 0.8294\n",
      "Epoch 3/40\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.35930 to 0.34228, saving model to keras_weights_GRU.hdf5\n",
      "928/928 - 145s - loss: 0.3486 - accuracy: 0.8359 - val_loss: 0.3423 - val_accuracy: 0.8401\n",
      "Epoch 4/40\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.34228 to 0.33439, saving model to keras_weights_GRU.hdf5\n",
      "928/928 - 144s - loss: 0.3364 - accuracy: 0.8430 - val_loss: 0.3344 - val_accuracy: 0.8448\n",
      "Epoch 5/40\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.33439 to 0.32603, saving model to keras_weights_GRU.hdf5\n",
      "928/928 - 145s - loss: 0.3282 - accuracy: 0.8476 - val_loss: 0.3260 - val_accuracy: 0.8494\n",
      "Epoch 6/40\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.32603\n",
      "928/928 - 145s - loss: 0.3218 - accuracy: 0.8511 - val_loss: 0.3288 - val_accuracy: 0.8483\n",
      "Epoch 7/40\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.32603 to 0.31577, saving model to keras_weights_GRU.hdf5\n",
      "928/928 - 144s - loss: 0.3162 - accuracy: 0.8543 - val_loss: 0.3158 - val_accuracy: 0.8553\n",
      "Epoch 8/40\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.31577 to 0.31297, saving model to keras_weights_GRU.hdf5\n",
      "928/928 - 145s - loss: 0.3114 - accuracy: 0.8570 - val_loss: 0.3130 - val_accuracy: 0.8570\n",
      "Epoch 9/40\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.31297 to 0.30837, saving model to keras_weights_GRU.hdf5\n",
      "928/928 - 144s - loss: 0.3075 - accuracy: 0.8591 - val_loss: 0.3084 - val_accuracy: 0.8598\n",
      "Epoch 10/40\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.30837 to 0.30517, saving model to keras_weights_GRU.hdf5\n",
      "928/928 - 144s - loss: 0.3038 - accuracy: 0.8610 - val_loss: 0.3052 - val_accuracy: 0.8613\n",
      "Epoch 11/40\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.30517 to 0.30218, saving model to keras_weights_GRU.hdf5\n",
      "928/928 - 144s - loss: 0.3005 - accuracy: 0.8631 - val_loss: 0.3022 - val_accuracy: 0.8629\n",
      "Epoch 12/40\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.30218 to 0.30124, saving model to keras_weights_GRU.hdf5\n",
      "928/928 - 145s - loss: 0.2972 - accuracy: 0.8650 - val_loss: 0.3012 - val_accuracy: 0.8640\n",
      "Epoch 13/40\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.30124 to 0.29700, saving model to keras_weights_GRU.hdf5\n",
      "928/928 - 144s - loss: 0.2944 - accuracy: 0.8664 - val_loss: 0.2970 - val_accuracy: 0.8654\n",
      "Epoch 14/40\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.29700 to 0.29569, saving model to keras_weights_GRU.hdf5\n",
      "928/928 - 144s - loss: 0.2920 - accuracy: 0.8680 - val_loss: 0.2957 - val_accuracy: 0.8665\n",
      "Epoch 15/40\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.29569 to 0.29367, saving model to keras_weights_GRU.hdf5\n",
      "928/928 - 144s - loss: 0.2893 - accuracy: 0.8693 - val_loss: 0.2937 - val_accuracy: 0.8672\n",
      "Epoch 16/40\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.29367 to 0.29077, saving model to keras_weights_GRU.hdf5\n",
      "928/928 - 145s - loss: 0.2871 - accuracy: 0.8705 - val_loss: 0.2908 - val_accuracy: 0.8689\n",
      "Epoch 17/40\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.29077 to 0.28976, saving model to keras_weights_GRU.hdf5\n",
      "928/928 - 144s - loss: 0.2849 - accuracy: 0.8717 - val_loss: 0.2898 - val_accuracy: 0.8698\n",
      "Epoch 18/40\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.28976 to 0.28920, saving model to keras_weights_GRU.hdf5\n",
      "928/928 - 144s - loss: 0.2829 - accuracy: 0.8727 - val_loss: 0.2892 - val_accuracy: 0.8701\n",
      "Epoch 19/40\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.28920 to 0.28715, saving model to keras_weights_GRU.hdf5\n",
      "928/928 - 144s - loss: 0.2807 - accuracy: 0.8740 - val_loss: 0.2872 - val_accuracy: 0.8712\n",
      "Epoch 20/40\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.28715 to 0.28388, saving model to keras_weights_GRU.hdf5\n",
      "928/928 - 144s - loss: 0.2788 - accuracy: 0.8750 - val_loss: 0.2839 - val_accuracy: 0.8728\n",
      "Epoch 21/40\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.28388 to 0.28280, saving model to keras_weights_GRU.hdf5\n",
      "928/928 - 145s - loss: 0.2771 - accuracy: 0.8759 - val_loss: 0.2828 - val_accuracy: 0.8736\n",
      "Epoch 22/40\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.28280 to 0.28166, saving model to keras_weights_GRU.hdf5\n",
      "928/928 - 144s - loss: 0.2754 - accuracy: 0.8768 - val_loss: 0.2817 - val_accuracy: 0.8739\n",
      "Epoch 23/40\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.28166\n",
      "928/928 - 144s - loss: 0.2739 - accuracy: 0.8777 - val_loss: 0.2822 - val_accuracy: 0.8738\n",
      "Epoch 24/40\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.28166 to 0.27853, saving model to keras_weights_GRU.hdf5\n",
      "928/928 - 144s - loss: 0.2721 - accuracy: 0.8787 - val_loss: 0.2785 - val_accuracy: 0.8764\n",
      "Epoch 25/40\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.27853 to 0.27755, saving model to keras_weights_GRU.hdf5\n",
      "928/928 - 145s - loss: 0.2707 - accuracy: 0.8795 - val_loss: 0.2775 - val_accuracy: 0.8767\n",
      "Epoch 26/40\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.27755 to 0.27623, saving model to keras_weights_GRU.hdf5\n",
      "928/928 - 144s - loss: 0.2694 - accuracy: 0.8801 - val_loss: 0.2762 - val_accuracy: 0.8768\n",
      "Epoch 27/40\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.27623 to 0.27415, saving model to keras_weights_GRU.hdf5\n",
      "928/928 - 144s - loss: 0.2681 - accuracy: 0.8808 - val_loss: 0.2741 - val_accuracy: 0.8781\n",
      "Epoch 28/40\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.27415 to 0.27356, saving model to keras_weights_GRU.hdf5\n",
      "928/928 - 144s - loss: 0.2667 - accuracy: 0.8817 - val_loss: 0.2736 - val_accuracy: 0.8789\n",
      "Epoch 29/40\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.27356\n",
      "928/928 - 145s - loss: 0.2654 - accuracy: 0.8823 - val_loss: 0.2751 - val_accuracy: 0.8780\n",
      "Epoch 30/40\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.27356 to 0.27157, saving model to keras_weights_GRU.hdf5\n",
      "928/928 - 145s - loss: 0.2643 - accuracy: 0.8828 - val_loss: 0.2716 - val_accuracy: 0.8802\n",
      "Epoch 31/40\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.27157 to 0.27059, saving model to keras_weights_GRU.hdf5\n",
      "928/928 - 145s - loss: 0.2633 - accuracy: 0.8835 - val_loss: 0.2706 - val_accuracy: 0.8802\n",
      "Epoch 32/40\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.27059 to 0.26999, saving model to keras_weights_GRU.hdf5\n",
      "928/928 - 144s - loss: 0.2619 - accuracy: 0.8841 - val_loss: 0.2700 - val_accuracy: 0.8805\n",
      "Epoch 33/40\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.26999 to 0.26968, saving model to keras_weights_GRU.hdf5\n",
      "928/928 - 144s - loss: 0.2610 - accuracy: 0.8846 - val_loss: 0.2697 - val_accuracy: 0.8809\n",
      "Epoch 34/40\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.26968 to 0.26954, saving model to keras_weights_GRU.hdf5\n",
      "928/928 - 145s - loss: 0.2603 - accuracy: 0.8849 - val_loss: 0.2695 - val_accuracy: 0.8811\n",
      "Epoch 35/40\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.26954 to 0.26736, saving model to keras_weights_GRU.hdf5\n",
      "928/928 - 144s - loss: 0.2591 - accuracy: 0.8855 - val_loss: 0.2674 - val_accuracy: 0.8819\n",
      "Epoch 36/40\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.26736 to 0.26684, saving model to keras_weights_GRU.hdf5\n",
      "928/928 - 144s - loss: 0.2582 - accuracy: 0.8862 - val_loss: 0.2668 - val_accuracy: 0.8827\n",
      "Epoch 37/40\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.26684 to 0.26587, saving model to keras_weights_GRU.hdf5\n",
      "928/928 - 144s - loss: 0.2573 - accuracy: 0.8866 - val_loss: 0.2659 - val_accuracy: 0.8829\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/40\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.26587\n",
      "928/928 - 145s - loss: 0.2564 - accuracy: 0.8870 - val_loss: 0.2678 - val_accuracy: 0.8823\n",
      "Epoch 39/40\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.26587\n",
      "928/928 - 144s - loss: 0.2556 - accuracy: 0.8875 - val_loss: 0.2660 - val_accuracy: 0.8833\n",
      "Epoch 40/40\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.26587 to 0.26544, saving model to keras_weights_GRU.hdf5\n",
      "928/928 - 144s - loss: 0.2550 - accuracy: 0.8878 - val_loss: 0.2654 - val_accuracy: 0.8837\n",
      "Model training time: 1:36:20.234368\n"
     ]
    }
   ],
   "source": [
    "training_start = datetime.datetime.now()\n",
    "model = create_model(embedding_matrix, maxlen, vocab_size)\n",
    "adam = keras.optimizers.Adam(lr = 0.001, decay = 0.0001)\n",
    "model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "earlyStopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=1, min_delta = 0.001)\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(filepath='keras_weights_GRU.hdf5', verbose=1, save_best_only=True)\n",
    "callbacks_list = [earlyStopping, checkpoint]\n",
    "\n",
    "model.fit(X_train, y_train, batch_size = 2048, epochs = 40, verbose=2, \n",
    "          validation_data=(X_val, y_val), callbacks = callbacks_list)\n",
    "training_stop = datetime.datetime.now()\n",
    "delta_training = training_stop - training_start\n",
    "print(\"Model training time:\", delta_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model evaluation time on training data 0:05:47.153900\n",
      "Model evaluation time on testing data 0:01:36.243978\n",
      "Training Loss: 0.254, Testing loss: 0.264\n",
      "Train: 0.888, Test: 0.884\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "before_train_evaluation = datetime.datetime.now()\n",
    "train_loss, train_acc = model.evaluate(X_train, y_train, verbose=0)\n",
    "after_train_evaluation = datetime.datetime.now()\n",
    "delta_train_evaluation = after_train_evaluation - before_train_evaluation\n",
    "print(\"Model evaluation time on training data\", delta_train_evaluation)\n",
    "\n",
    "before_test_evaluation = datetime.datetime.now()\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "after_test_evaluation = datetime.datetime.now()\n",
    "delta_test_evaluation = after_test_evaluation - before_test_evaluation\n",
    "print(\"Model evaluation time on testing data\", delta_test_evaluation)\n",
    "\n",
    "print('Training Loss: %.3f, Testing loss: %.3f' % (train_loss, test_loss) )\n",
    "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-a830a802c387>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'training_loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'validation_loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'loss'"
     ]
    }
   ],
   "source": [
    "# plot training history\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(model.history.history['loss'], label='training_loss')\n",
    "plt.plot(model.history.history['val_loss'], label='validation_loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.sequential.Sequential at 0x7fe27d9c63c8>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saving the model and loading the saved model\n",
    "\n",
    "model.save('../saved_models/full_model_CNN_LSTM_with_pca.h5')\n",
    "keras.models.load_model('../saved_models/full_model_CNN_LSTM_with_pca.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict(text_x):\n",
    "# \tx = np.zeros((1, maxlen), dtype=np.int)\n",
    "# \toffset = max(maxlen - len(text_x), 0)\n",
    "# \tfor t, char in enumerate(text_x):\n",
    "# \t    if t >= maxlen:\n",
    "# \t        break\n",
    "# \t    x[0, t + offset] = char_indices[char]\n",
    "#     pred = model.predict(x)\n",
    "# \treturn pred[0][0]\n",
    "\n",
    "\n",
    "# Like predict, but you pass in an array of URLs, and it is all\n",
    "# vectorized in one step, making it more efficient\n",
    "def predicts(text_X):\n",
    "    X = np.zeros((len(text_X), maxlen), dtype=np.int)\n",
    "    for i in range(len(text_X)):\n",
    "        offset = max(maxlen - len(text_X[i]), 0)\n",
    "        for t, char in enumerate(text_X[i]):\n",
    "            if t >= maxlen:\n",
    "                break\n",
    "            X[i, t + offset] = word_index[char]\n",
    "    preds = [pred[0] for pred in model.predict(X)]\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation\n",
    "# define a threshold value so that values below threshold will be classified as false_positive\n",
    "threshold = 0.5\n",
    "\n",
    "def evaluate_model(positives, negatives_train, negatives_dev, negatives_test, threshold):\n",
    "    false_negatives = 0.0\n",
    "    preds = predicts(positives)\n",
    "    for pred in preds:\n",
    "        if pred <= threshold:\n",
    "            false_negatives += 1\n",
    "    print(false_negatives / len(positives), \"false negatives for positives set.\")\n",
    "\n",
    "    false_positives_train = 0.0\n",
    "    preds = predicts(negatives_train)\n",
    "    for pred in preds:\n",
    "        if pred > threshold:\n",
    "            false_positives_train += 1\n",
    "\n",
    "    false_positives_dev = 0.0\n",
    "    preds = predicts(negatives_dev)\n",
    "    for pred in preds:\n",
    "        if pred > threshold:\n",
    "            false_positives_dev += 1\n",
    "\n",
    "    false_positives_test = 0.0\n",
    "    preds = predicts(negatives_test)\n",
    "    for pred in preds:\n",
    "        if pred > threshold:\n",
    "            false_positives_test += 1\n",
    "\n",
    "    print(false_positives_train / len(negatives_train), \"false positive rate for negative train.\")\n",
    "    print(false_positives_dev / len(negatives_dev), \"false positive rate for negative dev.\")\n",
    "    print(false_positives_test / len(negatives_test), \"false positive rate for negative test.\")\n",
    "\n",
    "evaluate_model(positives, negatives_train, negatives_dev, negatives_test, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting predictions on negative_dev set to find a suitable threshold value.\n",
    "\n",
    "# defining the false positive rate which we can change.\n",
    "fp_rate = 0.01\n",
    "\n",
    "print(\"Getting threshold for fp_rate\", fp_rate)\n",
    "preds = predicts(negatives_dev)\n",
    "preds.sort()\n",
    "fp_index = math.ceil((len(negatives_dev) * (1 - fp_rate)))\n",
    "threshold = preds[fp_index]\n",
    "\n",
    "print(\"Using threshold\", threshold) \n",
    "\n",
    "evaluate_model(positives, negatives_train, negatives_dev, negatives_test, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Adapted from https://www.geeksforgeeks.org/bloom-filters-introduction-and-python-implementation/ \n",
    "\n",
    "import math \n",
    "import mmh3 \n",
    "from bitarray import bitarray \n",
    "\n",
    "class BloomFilter(object): \n",
    "\n",
    "\t''' \n",
    "\tClass for Bloom filter, using murmur3 hash function \n",
    "\t'''\n",
    "\n",
    "\tdef __init__(self, items_count,fp_prob): \n",
    "\t\t''' \n",
    "\t\titems_count : int \n",
    "\t\t\tNumber of items expected to be stored in bloom filter \n",
    "\t\tfp_prob : float \n",
    "\t\t\tFalse Positive probability in decimal \n",
    "\t\t'''\n",
    "\t\t# False posible probability in decimal \n",
    "\t\tself.fp_prob = fp_prob \n",
    "\n",
    "\t\t# Size of bit array to use \n",
    "\t\tself.size = self.get_size(items_count,fp_prob) \n",
    "\n",
    "\t\t# number of hash functions to use \n",
    "\t\tself.hash_count = self.get_hash_count(self.size,items_count) \n",
    "\n",
    "\t\t# Bit array of given size \n",
    "\t\tself.bit_array = bitarray(self.size) \n",
    "        \n",
    "        # Return the size of bitarray in bytes\n",
    "        self.byte_size = self.bit_array.tobyte()\n",
    "\n",
    "\t\t# initialize all bits as 0 \n",
    "\t\tself.bit_array.setall(0) \n",
    "\n",
    "\tdef add(self, item): \n",
    "\t\t''' \n",
    "\t\tAdd an item in the filter \n",
    "\t\t'''\n",
    "\t\tdigests = [] \n",
    "\t\tfor i in range(self.hash_count): \n",
    "\n",
    "\t\t\t# create digest for given item. \n",
    "\t\t\t# i work as seed to mmh3.hash() function \n",
    "\t\t\t# With different seed, digest created is different \n",
    "\t\t\tdigest = mmh3.hash(item,i) % self.size \n",
    "\t\t\tdigests.append(digest) \n",
    "\n",
    "\t\t\t# set the bit True in bit_array \n",
    "\t\t\tself.bit_array[digest] = True\n",
    "\n",
    "\tdef check(self, item): \n",
    "\t\t''' \n",
    "\t\tCheck for existence of an item in filter \n",
    "\t\t'''\n",
    "\t\tfor i in range(self.hash_count): \n",
    "\t\t\tdigest = mmh3.hash(item,i) % self.size \n",
    "\t\t\tif self.bit_array[digest] == False: \n",
    "\n",
    "\t\t\t\t# if any of bit is False then,its not present \n",
    "\t\t\t\t# in filter \n",
    "\t\t\t\t# else there is probability that it exist \n",
    "\t\t\t\treturn False\n",
    "\t\treturn True\n",
    "\n",
    "\t@classmethod\n",
    "\tdef get_size(self,n,p): \n",
    "\t\t''' \n",
    "\t\tReturn the size of bit array(m) to used using \n",
    "\t\tfollowing formula \n",
    "\t\tm = -(n * lg(p)) / (lg(2)^2) \n",
    "\t\tn : int \n",
    "\t\t\tnumber of items expected to be stored in filter \n",
    "\t\tp : float \n",
    "\t\t\tFalse Positive probability in decimal \n",
    "\t\t'''\n",
    "\t\tm = -(n * math.log(p))/(math.log(2)**2) \n",
    "\t\treturn int(m)\n",
    "    \n",
    "\t@classmethod\n",
    "\tdef get_hash_count(self, m, n): \n",
    "\t\t''' \n",
    "\t\tReturn the hash function(k) to be used using \n",
    "\t\tfollowing formula \n",
    "\t\tk = (m/n) * lg(2) \n",
    "\n",
    "\t\tm : int \n",
    "\t\t\tsize of bit array \n",
    "\t\tn : int \n",
    "\t\t\tnumber of items expected to be stored in filter \n",
    "\t\t'''\n",
    "\t\tk = (m/n) * math.log(2) \n",
    "\t\treturn int(k) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the threshold value\n",
    "fp_rate = 0.01\n",
    "print(\"Getting threshold for fp_rate\", fp_rate)\n",
    "preds = predicts(negatives_dev)\n",
    "preds.sort()\n",
    "fp_index = math.ceil((len(negatives_dev) * (1 - fp_rate/2)))\n",
    "threshold = preds[fp_index]\n",
    "print(\"The threhold value to use is:\", threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bloom_filter(data):\n",
    "    print(\"Creating bloom filter\")\n",
    "    false_negatives = []\n",
    "    # calling the predicts function \n",
    "    preds = predicts(data)\n",
    "    for i in range(len(data)):\n",
    "        if preds[i] <= threshold:\n",
    "            false_negatives.append(data[i])\n",
    "    print(\"Number of false negatives at bloom time\", len(false_negatives))\n",
    "    bloom_filter = BloomFilter(len(false_negatives), fp_rate / 2)\n",
    "    for fn in false_negatives:\n",
    "        bloom_filter.add(fn)\n",
    "    print(\"Created bloom filter\")\n",
    "    return bloom_filter\n",
    "\n",
    "bloom_filter = create_bloom_filter(positives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict the output from the machine learning model\n",
    "def predict(text_x):\n",
    "    x = np.zeros((1, maxlen), dtype=np.int)\n",
    "    offset = max(maxlen - len(text_x), 0)\n",
    "    for t, char in enumerate(text_x):\n",
    "        if t >= maxlen:\n",
    "            break\n",
    "        x[0, t + offset] = word_index[char]\n",
    "    pred = model.predict(x)\n",
    "    return pred[0][0]\n",
    "\n",
    "\n",
    "def check_item(item):\n",
    "    if predict(item) > threshold:\n",
    "        return True\n",
    "    return bloom_filter.check(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Bloom filter bits needed\", bloom_filter.size)\n",
    "print(\"Bloom fiter size in bytes\", bloom_filter.byte_size)\n",
    "print(\"Hash functions needed\", bloom_filter.hash_count)\n",
    "    \n",
    "false_positives = 0.0\n",
    "for neg in negatives_test:\n",
    "    if check_item(neg):\n",
    "        false_positives += 1\n",
    "print(\"Test false positive rate: \", str(false_positives / len(negatives_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
