{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JXYYaehBU-dV"
   },
   "source": [
    "New LSTM Configuration **than** Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wr09UPPFweqA"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "import datetime\n",
    "from sklearn.decomposition import PCA\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 346
    },
    "colab_type": "code",
    "id": "qxe65Pah3t2B",
    "outputId": "d99db3cd-7820-45ec-a7d0-9acf448e1975"
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-e777c3117827>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mresolver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster_resolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPUClusterResolver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'grpc://'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'COLAB_TPU_ADDR'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_connect_to_cluster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresolver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# This is the TPU initialization code that has to be at the beginning.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/os.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0;31m# raise KeyError with the original key value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecodevalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'COLAB_TPU_ADDR'"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
    "tf.config.experimental_connect_to_cluster(resolver)\n",
    "# This is the TPU initialization code that has to be at the beginning.\n",
    "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "strategy = tf.distribute.experimental.TPUStrategy(resolver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CiGMbFlQwo83"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KhQl3sLKJN6X"
   },
   "outputs": [],
   "source": [
    "def data():\n",
    "    maxlen = 50\n",
    "    embedding_size = 50\n",
    "    \n",
    "    # importing the glove embeddings path \n",
    "    \n",
    "    embeddings_path = '../data/glove.6B.50d-char.txt'\n",
    "    \n",
    "    # Indexing character vectors using glove word vectors\n",
    "    embedding_vectors = {}\n",
    "    with open(embeddings_path, 'r') as f:\n",
    "        for line in f:\n",
    "            line_split = line.strip().split(\" \")\n",
    "            vec = np.array(line_split[1:], dtype=float)\n",
    "            char = line_split[0]\n",
    "            embedding_vectors[char] = vec\n",
    "#     print('Found %s char vectors.' % len(embedding_vectors))\n",
    "    \n",
    "    # loading the dataset\n",
    "    with open('../data/dataset.json', 'r') as f:\n",
    "        dataset = json.load(f)\n",
    "        positives = dataset['positives']\n",
    "        negatives = dataset['negatives']\n",
    "    \n",
    "    # dividing the dataset to make small models\n",
    "    data_fraction = 1\n",
    "    positives = positives[:int(data_fraction * len(positives))]\n",
    "    negatives = negatives[:int(data_fraction * len(negatives))]\n",
    "    \n",
    "    # Dividing the negatives dataset between train, dev and test\n",
    "    negatives_train = negatives[0: int(len(negatives) * .8)]\n",
    "    negatives_dev = negatives[int(len(negatives) * .8): int(len(negatives) * .9)]\n",
    "    negatives_test = negatives[int(len(negatives) * .9): ]\n",
    "    print(\"Split sizes:\")\n",
    "    print(len(positives), len(negatives_train), len(negatives_dev), len(negatives_test))\n",
    "    \n",
    "    # Shuffling the data\n",
    "    a = [(i, 0) for i in negatives_train]\n",
    "    b = [(i, 1) for i in positives]\n",
    "    combined = a + b\n",
    "    random.shuffle(combined)\n",
    "    shuffled = list(zip(*combined))\n",
    "    text_X = shuffled[0]\n",
    "    labels = shuffled[1]\n",
    "    \n",
    "    # tokenizing the input url's\n",
    "    tk = keras.preprocessing.text.Tokenizer(char_level=True)\n",
    "    tk.fit_on_texts(text_X)\n",
    "    \n",
    "    # List the vocabulary\n",
    "    word_index = tk.word_index\n",
    "    vocab_size = len(word_index) + 1\n",
    "#     print(vocab_size)\n",
    "#     print(word_index)\n",
    "    \n",
    "    # integer encode the documents\n",
    "    sequences = tk.texts_to_sequences(text_X)\n",
    "\n",
    "    # pad documents to a max length of 4 words\n",
    "    data = keras.preprocessing.sequence.pad_sequences(sequences, maxlen=maxlen) # by default the padding is post.\n",
    "    labels = np.asarray(labels)\n",
    "#     print('Shape of data tensor:', data.shape)\n",
    "#     print('Shape of label tensor:', labels.shape)\n",
    "    \n",
    "    # Dividing the dataset into train and test.\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # split the training data into a training set and a validation set\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "    \n",
    "    # create a weight matrix for words in training docs\n",
    "#     print('Preparing embedding matrix.')\n",
    "    embedding_matrix = np.zeros((vocab_size, 50))\n",
    "    for char, i in word_index.items():\n",
    "        embedding_vector = embedding_vectors.get(char)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "#     print(embedding_matrix.shape)\n",
    "    \n",
    "    # PCA Embedding dimension\n",
    "    pca_embedding_dim = 16\n",
    "    pca = PCA(n_components = pca_embedding_dim)\n",
    "    pca.fit(embedding_matrix[1:])\n",
    "    embedding_matrix_pca = np.array(pca.transform(embedding_matrix[1:]))\n",
    "    embedding_matrix_pca = np.insert(embedding_matrix_pca, 0, 0, axis=0)\n",
    "    print(\"PCA matrix created\")\n",
    "    \n",
    "    return (X_train, X_test, y_train, y_test, X_val, y_val, embedding_matrix, maxlen, vocab_size, word_index,\n",
    "            positives, negatives_train, negatives_dev, negatives_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mosbtoMxADIm"
   },
   "outputs": [],
   "source": [
    "def create_model(embedding_matrix, vocab_size, maxlen):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Embedding(vocab_size, 50, input_length=maxlen, weights=[embedding_matrix]))\n",
    "    model.add(keras.layers.LSTM(100, activation=\"tanh\", return_sequences=True))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Dropout(0.2))\n",
    "    model.add(keras.layers.LSTM(50, activation=\"tanh\"))\n",
    "    model.add(keras.layers.Dropout(0.2))\n",
    "    model.add(keras.layers.Dense(25, activation=\"tanh\"))\n",
    "    model.add(keras.layers.Dropout(0.2))\n",
    "    model.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fho2mcBFJnpB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split sizes:\n",
      "1491178 1148421 143553 143553\n",
      "PCA matrix created\n",
      "Data Preprocessing time: 0:01:04.491646\n"
     ]
    }
   ],
   "source": [
    "before_dataset = datetime.datetime.now()\n",
    "(X_train, X_test, y_train, y_test, X_val, y_val, embedding_matrix, maxlen, vocab_size, word_index,\n",
    "    positives, negatives_train, negatives_dev, negatives_test) = data()\n",
    "after_dataset = datetime.datetime.now()\n",
    "delta_dataset = after_dataset - before_dataset\n",
    "print(\"Data Preprocessing time:\", delta_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fz3G2XiIJrmU"
   },
   "outputs": [],
   "source": [
    "earlyStopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=1, min_delta = 0.001)\n",
    "file_path = '../model_weights/keras_weights_one_more_full_dataset_with_pca.hdf5'\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(filepath=file_path, verbose=1, save_best_only=True)\n",
    "callbacks_list = [earlyStopping, checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ldLFtyU0JvKd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 50, 50)            1950      \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 50, 100)           60400     \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 50, 100)           400       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 50, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 50)                30200     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 25)                1275      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 26        \n",
      "=================================================================\n",
      "Total params: 94,251\n",
      "Trainable params: 94,051\n",
      "Non-trainable params: 200\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/40\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.40941, saving model to ../model_weights/keras_weights_one_more_full_dataset_with_pca.hdf5\n",
      "928/928 - 582s - loss: 0.4926 - accuracy: 0.7378 - val_loss: 0.4094 - val_accuracy: 0.7961\n",
      "Epoch 2/40\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.40941 to 0.33076, saving model to ../model_weights/keras_weights_one_more_full_dataset_with_pca.hdf5\n",
      "928/928 - 869s - loss: 0.3787 - accuracy: 0.8178 - val_loss: 0.3308 - val_accuracy: 0.8460\n",
      "Epoch 3/40\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.33076 to 0.30212, saving model to ../model_weights/keras_weights_one_more_full_dataset_with_pca.hdf5\n",
      "928/928 - 886s - loss: 0.3260 - accuracy: 0.8505 - val_loss: 0.3021 - val_accuracy: 0.8652\n",
      "Epoch 4/40\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.30212 to 0.27678, saving model to ../model_weights/keras_weights_one_more_full_dataset_with_pca.hdf5\n",
      "928/928 - 885s - loss: 0.2984 - accuracy: 0.8665 - val_loss: 0.2768 - val_accuracy: 0.8792\n",
      "Epoch 5/40\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.27678 to 0.26213, saving model to ../model_weights/keras_weights_one_more_full_dataset_with_pca.hdf5\n",
      "928/928 - 881s - loss: 0.2805 - accuracy: 0.8767 - val_loss: 0.2621 - val_accuracy: 0.8873\n",
      "Epoch 6/40\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.26213 to 0.25118, saving model to ../model_weights/keras_weights_one_more_full_dataset_with_pca.hdf5\n",
      "928/928 - 873s - loss: 0.2680 - accuracy: 0.8833 - val_loss: 0.2512 - val_accuracy: 0.8920\n",
      "Epoch 7/40\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.25118 to 0.24151, saving model to ../model_weights/keras_weights_one_more_full_dataset_with_pca.hdf5\n",
      "928/928 - 873s - loss: 0.2583 - accuracy: 0.8888 - val_loss: 0.2415 - val_accuracy: 0.8956\n",
      "Epoch 8/40\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.24151 to 0.23589, saving model to ../model_weights/keras_weights_one_more_full_dataset_with_pca.hdf5\n",
      "928/928 - 870s - loss: 0.2506 - accuracy: 0.8927 - val_loss: 0.2359 - val_accuracy: 0.8996\n",
      "Epoch 9/40\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.23589 to 0.23095, saving model to ../model_weights/keras_weights_one_more_full_dataset_with_pca.hdf5\n",
      "928/928 - 880s - loss: 0.2440 - accuracy: 0.8961 - val_loss: 0.2310 - val_accuracy: 0.9017\n",
      "Epoch 10/40\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.23095 to 0.22689, saving model to ../model_weights/keras_weights_one_more_full_dataset_with_pca.hdf5\n",
      "928/928 - 873s - loss: 0.2390 - accuracy: 0.8987 - val_loss: 0.2269 - val_accuracy: 0.9041\n",
      "Epoch 11/40\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.22689 to 0.22314, saving model to ../model_weights/keras_weights_one_more_full_dataset_with_pca.hdf5\n",
      "928/928 - 869s - loss: 0.2343 - accuracy: 0.9012 - val_loss: 0.2231 - val_accuracy: 0.9060\n",
      "Epoch 12/40\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.22314 to 0.21984, saving model to ../model_weights/keras_weights_one_more_full_dataset_with_pca.hdf5\n",
      "928/928 - 872s - loss: 0.2303 - accuracy: 0.9031 - val_loss: 0.2198 - val_accuracy: 0.9078\n",
      "Epoch 13/40\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.21984 to 0.21792, saving model to ../model_weights/keras_weights_one_more_full_dataset_with_pca.hdf5\n",
      "928/928 - 869s - loss: 0.2267 - accuracy: 0.9050 - val_loss: 0.2179 - val_accuracy: 0.9078\n",
      "Epoch 14/40\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.21792 to 0.21490, saving model to ../model_weights/keras_weights_one_more_full_dataset_with_pca.hdf5\n",
      "928/928 - 674s - loss: 0.2239 - accuracy: 0.9062 - val_loss: 0.2149 - val_accuracy: 0.9102\n",
      "Epoch 15/40\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.21490 to 0.21347, saving model to ../model_weights/keras_weights_one_more_full_dataset_with_pca.hdf5\n",
      "928/928 - 647s - loss: 0.2208 - accuracy: 0.9078 - val_loss: 0.2135 - val_accuracy: 0.9109\n",
      "Epoch 16/40\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.21347 to 0.21148, saving model to ../model_weights/keras_weights_one_more_full_dataset_with_pca.hdf5\n",
      "928/928 - 560s - loss: 0.2183 - accuracy: 0.9090 - val_loss: 0.2115 - val_accuracy: 0.9116\n",
      "Epoch 17/40\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.21148 to 0.21113, saving model to ../model_weights/keras_weights_one_more_full_dataset_with_pca.hdf5\n",
      "928/928 - 607s - loss: 0.2159 - accuracy: 0.9101 - val_loss: 0.2111 - val_accuracy: 0.9122\n",
      "Epoch 18/40\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.21113 to 0.20895, saving model to ../model_weights/keras_weights_one_more_full_dataset_with_pca.hdf5\n",
      "928/928 - 599s - loss: 0.2139 - accuracy: 0.9109 - val_loss: 0.2090 - val_accuracy: 0.9133\n",
      "Epoch 19/40\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.20895 to 0.20731, saving model to ../model_weights/keras_weights_one_more_full_dataset_with_pca.hdf5\n",
      "928/928 - 539s - loss: 0.2115 - accuracy: 0.9122 - val_loss: 0.2073 - val_accuracy: 0.9137\n",
      "Epoch 20/40\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.20731\n",
      "928/928 - 539s - loss: 0.2097 - accuracy: 0.9130 - val_loss: 0.2089 - val_accuracy: 0.9131\n",
      "Epoch 21/40\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.20731 to 0.20564, saving model to ../model_weights/keras_weights_one_more_full_dataset_with_pca.hdf5\n",
      "928/928 - 536s - loss: 0.2076 - accuracy: 0.9140 - val_loss: 0.2056 - val_accuracy: 0.9148\n",
      "Epoch 22/40\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.20564 to 0.20420, saving model to ../model_weights/keras_weights_one_more_full_dataset_with_pca.hdf5\n",
      "928/928 - 536s - loss: 0.2064 - accuracy: 0.9147 - val_loss: 0.2042 - val_accuracy: 0.9151\n",
      "Epoch 23/40\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.20420 to 0.20195, saving model to ../model_weights/keras_weights_one_more_full_dataset_with_pca.hdf5\n",
      "928/928 - 537s - loss: 0.2042 - accuracy: 0.9156 - val_loss: 0.2020 - val_accuracy: 0.9157\n",
      "Epoch 24/40\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.20195 to 0.20150, saving model to ../model_weights/keras_weights_one_more_full_dataset_with_pca.hdf5\n",
      "928/928 - 535s - loss: 0.2029 - accuracy: 0.9163 - val_loss: 0.2015 - val_accuracy: 0.9168\n",
      "Epoch 25/40\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.20150 to 0.20069, saving model to ../model_weights/keras_weights_one_more_full_dataset_with_pca.hdf5\n",
      "928/928 - 536s - loss: 0.2013 - accuracy: 0.9169 - val_loss: 0.2007 - val_accuracy: 0.9171\n",
      "Epoch 26/40\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.20069 to 0.20059, saving model to ../model_weights/keras_weights_one_more_full_dataset_with_pca.hdf5\n",
      "928/928 - 535s - loss: 0.1998 - accuracy: 0.9175 - val_loss: 0.2006 - val_accuracy: 0.9173\n",
      "Epoch 27/40\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.20059 to 0.19990, saving model to ../model_weights/keras_weights_one_more_full_dataset_with_pca.hdf5\n",
      "928/928 - 538s - loss: 0.1986 - accuracy: 0.9182 - val_loss: 0.1999 - val_accuracy: 0.9174\n",
      "Epoch 28/40\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.19990 to 0.19930, saving model to ../model_weights/keras_weights_one_more_full_dataset_with_pca.hdf5\n",
      "928/928 - 642s - loss: 0.1972 - accuracy: 0.9188 - val_loss: 0.1993 - val_accuracy: 0.9180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/40\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.19930 to 0.19722, saving model to ../model_weights/keras_weights_one_more_full_dataset_with_pca.hdf5\n",
      "928/928 - 542s - loss: 0.1960 - accuracy: 0.9193 - val_loss: 0.1972 - val_accuracy: 0.9184\n",
      "Epoch 30/40\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.19722\n",
      "928/928 - 538s - loss: 0.1944 - accuracy: 0.9199 - val_loss: 0.1973 - val_accuracy: 0.9188\n",
      "Epoch 31/40\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.19722\n",
      "928/928 - 537s - loss: 0.1934 - accuracy: 0.9205 - val_loss: 0.1982 - val_accuracy: 0.9184\n",
      "Epoch 32/40\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.19722 to 0.19679, saving model to ../model_weights/keras_weights_one_more_full_dataset_with_pca.hdf5\n",
      "928/928 - 536s - loss: 0.1924 - accuracy: 0.9210 - val_loss: 0.1968 - val_accuracy: 0.9190\n",
      "Epoch 33/40\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.19679 to 0.19491, saving model to ../model_weights/keras_weights_one_more_full_dataset_with_pca.hdf5\n",
      "928/928 - 536s - loss: 0.1910 - accuracy: 0.9216 - val_loss: 0.1949 - val_accuracy: 0.9196\n",
      "Epoch 34/40\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.19491\n",
      "928/928 - 539s - loss: 0.1904 - accuracy: 0.9218 - val_loss: 0.2013 - val_accuracy: 0.9171\n",
      "Epoch 35/40\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.19491 to 0.19478, saving model to ../model_weights/keras_weights_one_more_full_dataset_with_pca.hdf5\n",
      "928/928 - 538s - loss: 0.1889 - accuracy: 0.9226 - val_loss: 0.1948 - val_accuracy: 0.9197\n",
      "Epoch 36/40\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.19478 to 0.19456, saving model to ../model_weights/keras_weights_one_more_full_dataset_with_pca.hdf5\n",
      "928/928 - 537s - loss: 0.1881 - accuracy: 0.9230 - val_loss: 0.1946 - val_accuracy: 0.9202\n",
      "Epoch 37/40\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.19456 to 0.19374, saving model to ../model_weights/keras_weights_one_more_full_dataset_with_pca.hdf5\n",
      "928/928 - 592s - loss: 0.1872 - accuracy: 0.9233 - val_loss: 0.1937 - val_accuracy: 0.9200\n",
      "Epoch 38/40\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.19374\n",
      "928/928 - 537s - loss: 0.1859 - accuracy: 0.9239 - val_loss: 0.1948 - val_accuracy: 0.9197\n",
      "Epoch 39/40\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.19374 to 0.19240, saving model to ../model_weights/keras_weights_one_more_full_dataset_with_pca.hdf5\n",
      "928/928 - 540s - loss: 0.1850 - accuracy: 0.9243 - val_loss: 0.1924 - val_accuracy: 0.9210\n",
      "Epoch 40/40\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.19240\n",
      "928/928 - 539s - loss: 0.1842 - accuracy: 0.9246 - val_loss: 0.1924 - val_accuracy: 0.9205\n",
      "Model training time: 7:16:26.439734\n"
     ]
    }
   ],
   "source": [
    "# optimizer = keras.optimizers.Adam(lr = 0.001, decay = 0.0001)\n",
    "# with strategy.scope():\n",
    "model = create_model(embedding_matrix, vocab_size, maxlen)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "training_start = datetime.datetime.now()\n",
    "model.fit(X_train, y_train, batch_size = 2048, epochs = 40, verbose=2, validation_data=(X_val, y_val), callbacks=callbacks_list)\n",
    "training_stop = datetime.datetime.now()\n",
    "delta_training = training_stop - training_start\n",
    "print(\"Model training time:\", delta_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xTfnt-wOweq-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model evaluation time on training data 0:14:02.386879\n",
      "Model evaluation time on testing data 0:03:54.166988\n",
      "Training Loss: 0.166, Testing loss: 0.193\n",
      "Train: 0.933, Test: 0.922\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "before_train_evaluation = datetime.datetime.now()\n",
    "train_loss, train_acc = model.evaluate(X_train, y_train, verbose=0)\n",
    "after_train_evaluation = datetime.datetime.now()\n",
    "delta_train_evaluation = after_train_evaluation - before_train_evaluation\n",
    "print(\"Model evaluation time on training data\", delta_train_evaluation)\n",
    "\n",
    "before_test_evaluation = datetime.datetime.now()\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "after_test_evaluation = datetime.datetime.now()\n",
    "delta_test_evaluation = after_test_evaluation - before_test_evaluation\n",
    "print(\"Model evaluation time on testing data\", delta_test_evaluation)\n",
    "\n",
    "print('Training Loss: %.3f, Testing loss: %.3f' % (train_loss, test_loss) )\n",
    "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-L99DutLwerC"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.sequential.Sequential at 0x7fb21d0da898>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saving the model and loading the saved model\n",
    "\n",
    "model.save(\"../saved_models/full_model_LSTM_without_pca_new_configuration.h5\")\n",
    "keras.models.load_model('../saved_models/full_model_LSTM_without_pca_new_configuration.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2RBP9h45werE"
   },
   "outputs": [],
   "source": [
    "# Like predict, but you pass in an array of URLs, and it is all\n",
    "# vectorized in one step, making it more efficient\n",
    "def predicts(text_X):\n",
    "    X = np.zeros((len(text_X), maxlen), dtype=np.int)\n",
    "    for i in range(len(text_X)):\n",
    "        offset = max(maxlen - len(text_X[i]), 0)\n",
    "        for t, char in enumerate(text_X[i]):\n",
    "            if t >= maxlen:\n",
    "                break\n",
    "            X[i, t + offset] = word_index[char]\n",
    "    preds = [pred[0] for pred in model.predict(X)]\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "odPiuDGZwerH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08359766573809431 false negatives for positives set.\n",
      "0.05354134067558848 false positive rate for negatives train.\n",
      "0.06262495384979763 false positive rate for negatives dev.\n",
      "0.06324493392684236 false positive rate for negatives test.\n"
     ]
    }
   ],
   "source": [
    "# Model Evaluation\n",
    "# define a threshold value so that values below threshold will be classified as false_positive\n",
    "threshold = 0.5\n",
    "\n",
    "def evaluate_model(positives, negatives_train, negatives_dev, negatives_test, threshold):\n",
    "    false_negatives = 0.0\n",
    "    preds = predicts(positives)\n",
    "    for pred in preds:\n",
    "        if pred <= threshold:\n",
    "            false_negatives += 1\n",
    "    print(false_negatives / len(positives), \"false negatives for positives set.\")\n",
    "\n",
    "    false_positives_train = 0.0\n",
    "    preds = predicts(negatives_train)\n",
    "    for pred in preds:\n",
    "        if pred > threshold:\n",
    "            false_positives_train += 1\n",
    "\n",
    "    false_positives_dev = 0.0\n",
    "    preds = predicts(negatives_dev)\n",
    "    for pred in preds:\n",
    "        if pred > threshold:\n",
    "            false_positives_dev += 1\n",
    "\n",
    "    false_positives_test = 0.0\n",
    "    preds = predicts(negatives_test)\n",
    "    for pred in preds:\n",
    "        if pred > threshold:\n",
    "            false_positives_test += 1\n",
    "\n",
    "    print(false_positives_train / len(negatives_train), \"false positive rate for negatives train.\")\n",
    "    print(false_positives_dev / len(negatives_dev), \"false positive rate for negatives dev.\")\n",
    "    print(false_positives_test / len(negatives_test), \"false positive rate for negatives test.\")\n",
    "\n",
    "evaluate_model(positives, negatives_train, negatives_dev, negatives_test, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nAHTdkVswerJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting threshold for fp_rate 0.01\n",
      "Using threshold 0.9402275\n",
      "0.20958597833390782 false negatives for positives set.\n",
      "0.0075878096969665305 false positive rate for negatives train.\n",
      "0.009989341915529457 false positive rate for negatives dev.\n",
      "0.009891816959589838 false positive rate for negatives test.\n"
     ]
    }
   ],
   "source": [
    "# Getting predictions on negative_dev set to find a suitable threshold value.\n",
    "\n",
    "# defining the false positive rate which we can change.\n",
    "fp_rate = 0.01\n",
    "\n",
    "print(\"Getting threshold for fp_rate\", fp_rate)\n",
    "preds = predicts(negatives_dev)\n",
    "preds.sort()\n",
    "fp_index = math.ceil((len(negatives_dev) * (1 - fp_rate)))\n",
    "threshold = preds[fp_index]\n",
    "\n",
    "print(\"Using threshold\", threshold) \n",
    "\n",
    "evaluate_model(positives, negatives_train, negatives_dev, negatives_test, threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1LR9k5fEbP2u"
   },
   "source": [
    "### Bloom Filter Implementation Using Murmur Hash Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3ms78DNCwerM"
   },
   "outputs": [],
   "source": [
    "##  Adapted from https://www.geeksforgeeks.org/bloom-filters-introduction-and-python-implementation/ \n",
    "\n",
    "import math \n",
    "import mmh3 \n",
    "from bitarray import bitarray \n",
    "\n",
    "class BloomFilter(object): \n",
    "    \n",
    "    ''' \n",
    "    Class for Bloom filter, using murmur3 hash function \n",
    "    '''\n",
    "    \n",
    "    def __init__(self, items_count,fp_prob): \n",
    "        ''' \n",
    "        items_count : int \n",
    "            Number of items expected to be stored in bloom filter \n",
    "        fp_prob : float \n",
    "            False Positive probability in decimal \n",
    "        '''\n",
    "        # False posible probability in decimal \n",
    "        self.fp_prob = fp_prob \n",
    "        \n",
    "        # Size of bit array to use \n",
    "        self.size = self.get_size(items_count,fp_prob) \n",
    "        \n",
    "        # number of hash functions to use \n",
    "        self.hash_count = self.get_hash_count(self.size,items_count) \n",
    "        \n",
    "        # Bit array of given size \n",
    "        self.bit_array = bitarray(self.size) \n",
    "                \n",
    "        # initialize all bits as 0 \n",
    "        self.bit_array.setall(0) \n",
    "\n",
    "    def add(self, item): \n",
    "        ''' \n",
    "        Add an item in the filter \n",
    "        '''\n",
    "        digests = [] \n",
    "        for i in range(self.hash_count): \n",
    "            \n",
    "            # create digest for given item. \n",
    "            # i work as seed to mmh3.hash() function \n",
    "            # With different seed, digest created is different \n",
    "            digest = mmh3.hash(item,i) % self.size \n",
    "            digests.append(digest) \n",
    "\n",
    "            # set the bit True in bit_array \n",
    "            self.bit_array[digest] = True\n",
    "\n",
    "    def check(self, item): \n",
    "        ''' \n",
    "        Check for existence of an item in filter \n",
    "        '''\n",
    "        for i in range(self.hash_count): \n",
    "            digest = mmh3.hash(item,i) % self.size \n",
    "            if self.bit_array[digest] == False: \n",
    "\n",
    "                # if any of bit is False then,its not present \n",
    "                # in filter \n",
    "                # else there is probability that it exist \n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    @classmethod\n",
    "    def get_size(self,n,p): \n",
    "        ''' \n",
    "        Return the size of bit array(m) to used using \n",
    "        following formula \n",
    "        m = -(n * lg(p)) / (lg(2)^2) \n",
    "        n : int \n",
    "            number of items expected to be stored in filter \n",
    "        p : float \n",
    "            False Positive probability in decimal \n",
    "        '''\n",
    "        m = -(n * math.log(p))/(math.log(2)**2) \n",
    "        return int(m) \n",
    "\n",
    "    @classmethod\n",
    "    def get_hash_count(self, m, n): \n",
    "        ''' \n",
    "        Return the hash function(k) to be used using \n",
    "        following formula \n",
    "        k = (m/n) * lg(2) \n",
    "        \n",
    "        m : int \n",
    "            size of bit array \n",
    "        n : int \n",
    "            number of items expected to be stored in filter \n",
    "        '''\n",
    "        k = (m/n) * math.log(2) \n",
    "        return int(k) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GZ7bYhRFwerP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating bloom filter\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-b567f177ac2e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbloom_filter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mbloom_filter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_bloom_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositives\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-b567f177ac2e>\u001b[0m in \u001b[0;36mcreate_bloom_filter\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mfalse_negatives\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# calling the predicts function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredicts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-7d3f8920f0f2>\u001b[0m in \u001b[0;36mpredicts\u001b[0;34m(text_X)\u001b[0m\n\u001b[1;32m      9\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/research_data/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m       raise ValueError('{} is not supported in multi-worker mode.'.format(\n\u001b[1;32m     87\u001b[0m           method.__name__))\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m   return tf_decorator.make_decorator(\n",
      "\u001b[0;32m/research_data/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1266\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1267\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1268\u001b[0;31m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1269\u001b[0m             \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1270\u001b[0m             \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/research_data/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/research_data/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    616\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 618\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    619\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m/research_data/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/research_data/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/research_data/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/research_data/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/research_data/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def create_bloom_filter(data):\n",
    "    print(\"Creating bloom filter\")\n",
    "    false_negatives = []\n",
    "    # calling the predicts function \n",
    "    preds = predicts(data)\n",
    "    for i in range(len(data)):\n",
    "        if preds[i] <= threshold:\n",
    "            false_negatives.append(data[i])\n",
    "    print(\"Number of false negatives at bloom time\", len(false_negatives))\n",
    "    bloom_filter = BloomFilter(len(false_negatives), fp_rate / 2)\n",
    "    for fn in false_negatives:\n",
    "        bloom_filter.add(fn)\n",
    "    print(\"Created bloom filter\")\n",
    "    return bloom_filter\n",
    "\n",
    "bloom_filter = create_bloom_filter(positives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n2KC2YTYwerR"
   },
   "outputs": [],
   "source": [
    "# Function to predict the output from the machine learning model\n",
    "def predict(text_x):\n",
    "    x = np.zeros((1, maxlen), dtype=np.int)\n",
    "    offset = max(maxlen - len(text_x), 0)\n",
    "    for t, char in enumerate(text_x):\n",
    "        if t >= maxlen:\n",
    "            break\n",
    "        x[0, t + offset] = word_index[char]\n",
    "    pred = model.predict(x)\n",
    "    return pred[0][0]\n",
    "\n",
    "def check_item(item):\n",
    "    if predict(item) > threshold:\n",
    "        return True\n",
    "    return bloom_filter.check(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-DNkYxbUwerT"
   },
   "outputs": [],
   "source": [
    "print(\"Bloom filter bits needed\", bloom_filter.size)\n",
    "print(\"Bloom filter size in bytes\", (bloom_filter.size)/8)\n",
    "print(\"Number of hash\", bloom_filter.hash_count)\n",
    "    \n",
    "false_positives = 0.0\n",
    "for neg in negatives_test:\n",
    "    if check_item(neg):\n",
    "        false_positives += 1\n",
    "print(\"Test false positive rate: \", str(false_positives / len(negatives_test)))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
