{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import math\n",
    "import datetime\n",
    "from collections import Counter\n",
    "import matplotlib as pyplot\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Tensorflow and keras imports\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0-dev20200522\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "K.clear_session()\n",
    "tf.compat.v1.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data():\n",
    "    maxlen = 50\n",
    "    embedding_size = 50\n",
    "    \n",
    "    # importing the glove embeddings path \n",
    "    embeddings_path = '../data/glove.6B.50d-char.txt'\n",
    "    \n",
    "    # Indexing character vectors using glove word vectors\n",
    "    embedding_vectors = {}\n",
    "    with open(embeddings_path, 'r') as f:\n",
    "        for line in f:\n",
    "            line_split = line.strip().split(\" \")\n",
    "            vec = np.array(line_split[1:], dtype=float)\n",
    "            char = line_split[0]\n",
    "            embedding_vectors[char] = vec\n",
    "#     print('Found %s char vectors.' % len(embedding_vectors))\n",
    "    \n",
    "    # loading the dataset\n",
    "    with open('../data/dataset.json', 'r') as f:\n",
    "        dataset = json.load(f)\n",
    "        positives = dataset['positives']\n",
    "        negatives = dataset['negatives']\n",
    "    \n",
    "    # dividing the dataset to make small models\n",
    "    data_fraction = 0.5\n",
    "    positives = positives[:int(data_fraction * len(positives))]\n",
    "    negatives = negatives[:int(data_fraction * len(negatives))]\n",
    "    \n",
    "    # Dividing the negatives dataset between train, dev and test\n",
    "    negatives_train = negatives[0: int(len(negatives) * .8)]\n",
    "    negatives_dev = negatives[int(len(negatives) * .8): int(len(negatives) * .9)]\n",
    "    negatives_test = negatives[int(len(negatives) * .9): ]\n",
    "    print(\"Split sizes:\")\n",
    "    print(len(positives), len(negatives_train), len(negatives_dev), len(negatives_test))\n",
    "    \n",
    "    # Shuffling the data\n",
    "    a = [(i, 0) for i in negatives_train]\n",
    "    b = [(i, 1) for i in positives]\n",
    "    combined = a + b\n",
    "    random.shuffle(combined)\n",
    "    shuffled = list(zip(*combined))\n",
    "    text_X = shuffled[0]\n",
    "    labels = shuffled[1]\n",
    "    \n",
    "    # tokenizing the input url's\n",
    "    tk = keras.preprocessing.text.Tokenizer(char_level=True)\n",
    "    tk.fit_on_texts(text_X)\n",
    "    \n",
    "    # List the vocabulary\n",
    "    word_index = tk.word_index\n",
    "    vocab_size = len(word_index) + 1\n",
    "    \n",
    "    # integer encode the documents\n",
    "    sequences = tk.texts_to_sequences(text_X)\n",
    "\n",
    "    # pad documents to a max length of 4 words\n",
    "    data = keras.preprocessing.sequence.pad_sequences(sequences, maxlen=maxlen) # by default the padding is post.\n",
    "    labels = np.asarray(labels)\n",
    "    \n",
    "    # Dividing the dataset into train and test.\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # split the training data into a training set and a validation set\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "    \n",
    "    # create a weight matrix for words in training docs\n",
    "    embedding_matrix = np.zeros((vocab_size, 50))\n",
    "    for char, i in word_index.items():\n",
    "        embedding_vector = embedding_vectors.get(char)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    \n",
    "    # PCA Embedding dimension\n",
    "    pca_embedding_dim = 16\n",
    "    pca = PCA(n_components = pca_embedding_dim)\n",
    "    pca.fit(embedding_matrix[1:])\n",
    "    embedding_matrix_pca = np.array(pca.transform(embedding_matrix[1:]))\n",
    "    embedding_matrix_pca = np.insert(embedding_matrix_pca, 0, 0, axis=0)\n",
    "    print(\"PCA matrix created\")\n",
    "    \n",
    "    return (X_train, X_test, y_train, y_test, X_val, y_val, embedding_matrix_pca, maxlen, vocab_size,\n",
    "            positives, negatives_train, negatives_dev, negatives_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define GRU Model\n",
    "\n",
    "##### Defining embedding layer\n",
    "    Defining the embedding layer as the first layer of the model and using the original embedding dimension and not the reduced dimensions calculated using PCA. \n",
    "\n",
    "1. Use weights= embedding_matrix, if using the original embedding size else,\n",
    "2. Use weights= embedding_matrix_pca if using PCA.\n",
    "\n",
    "##### Defining a GRU layer with necessary arguments\n",
    "\n",
    "1. argument_1 = gru_size\n",
    "2. argument_2 = return sequences is False if there is no second gru layer, True otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(embedding_matrix, vocab_size, maxlen):\n",
    "    model = keras.Sequential([\n",
    "            keras.layers.Embedding(vocab_size, 16, input_length=50, weights=[embedding_matrix]),\n",
    "            keras.layers.GRU(16, return_sequences = False),\n",
    "            keras.layers.Dense(8, activation='relu'),\n",
    "            keras.layers.Dense(1, activation = 'sigmoid')\n",
    "    ])\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(lr = 0.001, decay = 0.0001)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    earlyStopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=1, min_delta = 0.001)\n",
    "    file_path = '../model_weights/keras_weights_GRU.hdf5'\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(filepath=file_path, verbose=1, save_best_only=True)\n",
    "    callbacks_list = [earlyStopping, checkpoint]\n",
    "    print(model.summary())\n",
    "    return model, callbacks_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split sizes:\n",
      "745589 574210 71776 71777\n",
      "PCA matrix created\n",
      "Data Preprocessing time: 0:00:34.832845\n"
     ]
    }
   ],
   "source": [
    "before_dataset = datetime.datetime.now()\n",
    "(X_train, X_test, y_train, y_test, X_val, y_val, embedding_matrix, maxlen, vocab_size, \n",
    "    positives, negatives_train, negatives_dev, negatives_test) = data()\n",
    "after_dataset = datetime.datetime.now()\n",
    "delta_dataset = after_dataset - before_dataset\n",
    "print(\"Data Preprocessing time:\", delta_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 50, 16)            624       \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, 16)                1632      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 2,401\n",
      "Trainable params: 2,401\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.53679, saving model to ../model_weights/keras_weights_GRU.hdf5\n",
      "464/464 - 37s - loss: 0.5832 - accuracy: 0.6652 - val_loss: 0.5368 - val_accuracy: 0.7074\n",
      "Epoch 2/20\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.53679 to 0.49833, saving model to ../model_weights/keras_weights_GRU.hdf5\n",
      "464/464 - 37s - loss: 0.5171 - accuracy: 0.7212 - val_loss: 0.4983 - val_accuracy: 0.7374\n",
      "Epoch 3/20\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.49833 to 0.48063, saving model to ../model_weights/keras_weights_GRU.hdf5\n",
      "464/464 - 37s - loss: 0.4901 - accuracy: 0.7425 - val_loss: 0.4806 - val_accuracy: 0.7507\n",
      "Epoch 4/20\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.48063 to 0.46687, saving model to ../model_weights/keras_weights_GRU.hdf5\n",
      "464/464 - 37s - loss: 0.4746 - accuracy: 0.7542 - val_loss: 0.4669 - val_accuracy: 0.7594\n",
      "Epoch 5/20\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.46687 to 0.45828, saving model to ../model_weights/keras_weights_GRU.hdf5\n",
      "464/464 - 37s - loss: 0.4626 - accuracy: 0.7624 - val_loss: 0.4583 - val_accuracy: 0.7645\n",
      "Epoch 6/20\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.45828 to 0.44304, saving model to ../model_weights/keras_weights_GRU.hdf5\n",
      "464/464 - 37s - loss: 0.4509 - accuracy: 0.7701 - val_loss: 0.4430 - val_accuracy: 0.7752\n",
      "Epoch 7/20\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.44304 to 0.43550, saving model to ../model_weights/keras_weights_GRU.hdf5\n",
      "464/464 - 37s - loss: 0.4407 - accuracy: 0.7771 - val_loss: 0.4355 - val_accuracy: 0.7804\n",
      "Epoch 8/20\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.43550 to 0.42883, saving model to ../model_weights/keras_weights_GRU.hdf5\n",
      "464/464 - 37s - loss: 0.4320 - accuracy: 0.7836 - val_loss: 0.4288 - val_accuracy: 0.7870\n",
      "Epoch 9/20\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.42883 to 0.42064, saving model to ../model_weights/keras_weights_GRU.hdf5\n",
      "464/464 - 37s - loss: 0.4255 - accuracy: 0.7884 - val_loss: 0.4206 - val_accuracy: 0.7929\n",
      "Epoch 10/20\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.42064 to 0.41477, saving model to ../model_weights/keras_weights_GRU.hdf5\n",
      "464/464 - 37s - loss: 0.4200 - accuracy: 0.7924 - val_loss: 0.4148 - val_accuracy: 0.7962\n",
      "Epoch 11/20\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.41477 to 0.41106, saving model to ../model_weights/keras_weights_GRU.hdf5\n",
      "464/464 - 37s - loss: 0.4145 - accuracy: 0.7961 - val_loss: 0.4111 - val_accuracy: 0.7998\n",
      "Epoch 12/20\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.41106 to 0.40460, saving model to ../model_weights/keras_weights_GRU.hdf5\n",
      "464/464 - 37s - loss: 0.4093 - accuracy: 0.7997 - val_loss: 0.4046 - val_accuracy: 0.8032\n",
      "Epoch 13/20\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.40460 to 0.40164, saving model to ../model_weights/keras_weights_GRU.hdf5\n",
      "464/464 - 37s - loss: 0.4041 - accuracy: 0.8031 - val_loss: 0.4016 - val_accuracy: 0.8051\n",
      "Epoch 14/20\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.40164 to 0.39631, saving model to ../model_weights/keras_weights_GRU.hdf5\n",
      "464/464 - 37s - loss: 0.3989 - accuracy: 0.8063 - val_loss: 0.3963 - val_accuracy: 0.8089\n",
      "Epoch 15/20\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.39631 to 0.39032, saving model to ../model_weights/keras_weights_GRU.hdf5\n",
      "464/464 - 37s - loss: 0.3944 - accuracy: 0.8094 - val_loss: 0.3903 - val_accuracy: 0.8131\n",
      "Epoch 16/20\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.39032 to 0.38566, saving model to ../model_weights/keras_weights_GRU.hdf5\n",
      "464/464 - 37s - loss: 0.3899 - accuracy: 0.8123 - val_loss: 0.3857 - val_accuracy: 0.8149\n",
      "Epoch 17/20\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.38566 to 0.38351, saving model to ../model_weights/keras_weights_GRU.hdf5\n",
      "464/464 - 37s - loss: 0.3861 - accuracy: 0.8147 - val_loss: 0.3835 - val_accuracy: 0.8160\n",
      "Epoch 18/20\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.38351 to 0.37945, saving model to ../model_weights/keras_weights_GRU.hdf5\n",
      "464/464 - 37s - loss: 0.3825 - accuracy: 0.8172 - val_loss: 0.3794 - val_accuracy: 0.8183\n",
      "Epoch 19/20\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.37945 to 0.37748, saving model to ../model_weights/keras_weights_GRU.hdf5\n",
      "464/464 - 37s - loss: 0.3794 - accuracy: 0.8190 - val_loss: 0.3775 - val_accuracy: 0.8203\n",
      "Epoch 20/20\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.37748 to 0.37452, saving model to ../model_weights/keras_weights_GRU.hdf5\n",
      "464/464 - 37s - loss: 0.3762 - accuracy: 0.8211 - val_loss: 0.3745 - val_accuracy: 0.8213\n",
      "Model training time: 0:12:21.228399\n"
     ]
    }
   ],
   "source": [
    "training_start = datetime.datetime.now()\n",
    "model, callbacks_list = create_model(embedding_matrix, vocab_size, maxlen)\n",
    "history = model.fit(X_train, y_train, batch_size = 2048, epochs = 20, verbose=2, \n",
    "          validation_data=(X_val, y_val), callbacks = callbacks_list)\n",
    "training_stop = datetime.datetime.now()\n",
    "delta_training = training_stop - training_start\n",
    "print(\"Model training time:\", delta_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model evaluation time on training data 0:02:30.279750\n",
      "Model evaluation time on testing data 0:00:41.856767\n",
      "Training Loss: 0.352, Testing loss: 0.354\n",
      "Train: 0.835, Test: 0.834\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "before_train_evaluation = datetime.datetime.now()\n",
    "train_loss, train_acc = model.evaluate(X_train, y_train, verbose=0)\n",
    "after_train_evaluation = datetime.datetime.now()\n",
    "delta_train_evaluation = after_train_evaluation - before_train_evaluation\n",
    "print(\"Model evaluation time on training data\", delta_train_evaluation)\n",
    "\n",
    "before_test_evaluation = datetime.datetime.now()\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "after_test_evaluation = datetime.datetime.now()\n",
    "delta_test_evaluation = after_test_evaluation - before_test_evaluation\n",
    "print(\"Model evaluation time on testing data\", delta_test_evaluation)\n",
    "\n",
    "print('Training Loss: %.3f, Testing loss: %.3f' % (train_loss, test_loss) )\n",
    "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training history\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.sequential.Sequential at 0x7fe46293f0f0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saving the model and loading the saved model\n",
    "\n",
    "model.save('../saved_models/model_GRU_PCA_prune.h5')\n",
    "keras.models.load_model('../saved_models/model_GRU_PCA_prune.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post Training Quantization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpo_a5vnmd/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpo_a5vnmd/assets\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "# converter.experimental_new_converter = True\n",
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write out it to a tflite file\n",
    "import pathlib\n",
    "\n",
    "tflite_models_dir = pathlib.Path(\"/tmp/mnist_tflite_models/\")\n",
    "tflite_models_dir.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21156"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tflite_model_file = tflite_models_dir/\"mnist_model.tflite\"\n",
    "tflite_model_file.write_bytes(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpe4xaz8jb/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpe4xaz8jb/assets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "22176"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\n",
    "tflite_quant_model = converter.convert()\n",
    "tflite_model_quant_file = tflite_models_dir/\"mnist_model_quant.tflite\"\n",
    "tflite_model_quant_file.write_bytes(tflite_quant_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 48K\r\n",
      "-rw-rw-r-- 1 ubuntu ubuntu 22K May 22 17:48 mnist_model_quant.tflite\r\n",
      "-rw-rw-r-- 1 ubuntu ubuntu 21K May 22 17:44 mnist_model.tflite\r\n"
     ]
    }
   ],
   "source": [
    "ls -lh {tflite_models_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning - Trim Insignificant Weights\n",
    "\n",
    "Magnitude-based weight pruning gradually zeroes out model weights during the training process to achieve model sparsity. Sparse models are easier to compress, and we can skip the zeroes during inference for latency improvements.\n",
    "\n",
    "This technique brings improvements via model compression. In the future, framework support for this technique will provide latency improvements. We've seen up to 6x improvements in model compression with minimal loss of accuracy.\n",
    "\n",
    "The technique is being evaluated in various speech applications, such as speech recognition and text-to-speech, and has been experimented on across various vision and translation models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import os\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline test accuracy: 0.8394188\n",
      "Saved baseline model to: /tmp/tmpgtmlehn4.h5\n"
     ]
    }
   ],
   "source": [
    "# Evaluate baseline model accuracy and saving it for later usage\n",
    "\n",
    "_, baseline_model_accuracy = model.evaluate(\n",
    "    X_test, y_test, verbose=0)\n",
    "\n",
    "print('Baseline test accuracy:', baseline_model_accuracy)\n",
    "\n",
    "_, keras_file = tempfile.mkstemp('.h5')\n",
    "model.save(keras_file, include_optimizer=False)\n",
    "print('Saved baseline model to:', keras_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tune pre-trained model with pruning  \n",
    "Define the model  \n",
    "You will apply pruning to the whole model and see this in the model summary.\n",
    "\n",
    "In this example, you start the model with 50% sparsity (50% zeros in weights) and end with 80% sparsity.\n",
    "\n",
    "In the comprehensive guide, you can see how to prune some layers for model accuracy improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "prune_low_magnitude_embeddin (None, 50, 16)            1250      \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_gru (Pru (None, 16)                3171      \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_dense (P (None, 8)                 266       \n",
      "_________________________________________________________________\n",
      "prune_low_magnitude_dense_1  (None, 1)                 19        \n",
      "=================================================================\n",
      "Total params: 4,706\n",
      "Trainable params: 2,401\n",
      "Non-trainable params: 2,305\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
    "\n",
    "# Compute end step to finish pruning after 2 epochs.\n",
    "batch_size = 2048\n",
    "epochs = 50\n",
    "validation_split = 0.1 # 10% of training set will be used for validation set. \n",
    "\n",
    "num_train = X_train.shape[0] * (1 - validation_split)\n",
    "end_step = np.ceil(num_train / batch_size).astype(np.int32) * epochs\n",
    "\n",
    "# Define model for pruning.\n",
    "pruning_params = {'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.30,\n",
    "                                                               final_sparsity=0.80,\n",
    "                                                               begin_step=0,\n",
    "                                                               end_step=end_step)\n",
    "}\n",
    "\n",
    "model_for_pruning = prune_low_magnitude(model, **pruning_params)\n",
    "optimizer = keras.optimizers.Adam(lr = 0.001, decay = 0.0001)\n",
    "# `prune_low_magnitude` requires a recompile.\n",
    "model_for_pruning.compile(optimizer=optimizer,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_for_pruning.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and evaluate the model against baseline  \n",
    "Fine tune with pruning for two epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 855229 samples, validate on 95026 samples\n",
      "Epoch 1/50\n",
      "  2048/855229 [..............................] - ETA: 24:56 - loss: 0.3514 - accuracy: 0.8320WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.204280). Check your callbacks.\n",
      "855229/855229 [==============================] - 42s 49us/sample - loss: 0.3511 - accuracy: 0.8355 - val_loss: 0.3515 - val_accuracy: 0.8357\n",
      "Epoch 2/50\n",
      "855229/855229 [==============================] - 37s 43us/sample - loss: 0.3485 - accuracy: 0.8370 - val_loss: 0.3523 - val_accuracy: 0.8360\n",
      "Epoch 3/50\n",
      "855229/855229 [==============================] - 37s 43us/sample - loss: 0.3467 - accuracy: 0.8377 - val_loss: 0.3483 - val_accuracy: 0.8382\n",
      "Epoch 4/50\n",
      "855229/855229 [==============================] - 37s 43us/sample - loss: 0.3456 - accuracy: 0.8383 - val_loss: 0.3479 - val_accuracy: 0.8382\n",
      "Epoch 5/50\n",
      "855229/855229 [==============================] - 37s 43us/sample - loss: 0.3455 - accuracy: 0.8387 - val_loss: 0.3472 - val_accuracy: 0.8385\n",
      "Epoch 6/50\n",
      "855229/855229 [==============================] - 37s 43us/sample - loss: 0.3494 - accuracy: 0.8367 - val_loss: 0.3528 - val_accuracy: 0.8355\n",
      "Epoch 7/50\n",
      "855229/855229 [==============================] - 37s 43us/sample - loss: 0.3510 - accuracy: 0.8356 - val_loss: 0.3544 - val_accuracy: 0.8341\n",
      "Epoch 8/50\n",
      "855229/855229 [==============================] - 37s 43us/sample - loss: 0.3543 - accuracy: 0.8339 - val_loss: 0.3601 - val_accuracy: 0.8314\n",
      "Epoch 9/50\n",
      "855229/855229 [==============================] - 37s 43us/sample - loss: 0.3639 - accuracy: 0.8285 - val_loss: 0.3660 - val_accuracy: 0.8268\n",
      "Epoch 10/50\n",
      "855229/855229 [==============================] - 37s 43us/sample - loss: 0.3683 - accuracy: 0.8255 - val_loss: 0.3692 - val_accuracy: 0.8258\n",
      "Epoch 11/50\n",
      "855229/855229 [==============================] - 37s 43us/sample - loss: 0.3707 - accuracy: 0.8250 - val_loss: 0.3720 - val_accuracy: 0.8232\n",
      "Epoch 12/50\n",
      "855229/855229 [==============================] - 37s 44us/sample - loss: 0.3815 - accuracy: 0.8194 - val_loss: 0.3809 - val_accuracy: 0.8185\n",
      "Epoch 13/50\n",
      "855229/855229 [==============================] - 37s 43us/sample - loss: 0.3786 - accuracy: 0.8202 - val_loss: 0.3813 - val_accuracy: 0.8188\n",
      "Epoch 14/50\n",
      "855229/855229 [==============================] - 37s 44us/sample - loss: 0.3825 - accuracy: 0.8167 - val_loss: 0.3864 - val_accuracy: 0.8143\n",
      "Epoch 15/50\n",
      "855229/855229 [==============================] - 37s 44us/sample - loss: 0.3951 - accuracy: 0.8093 - val_loss: 0.4104 - val_accuracy: 0.7998\n",
      "Epoch 16/50\n",
      "855229/855229 [==============================] - 37s 44us/sample - loss: 0.4076 - accuracy: 0.8009 - val_loss: 0.4050 - val_accuracy: 0.8025\n",
      "Epoch 17/50\n",
      "855229/855229 [==============================] - 37s 43us/sample - loss: 0.4077 - accuracy: 0.8008 - val_loss: 0.4219 - val_accuracy: 0.7926\n",
      "Epoch 18/50\n",
      "855229/855229 [==============================] - 37s 43us/sample - loss: 0.4179 - accuracy: 0.7945 - val_loss: 0.4474 - val_accuracy: 0.7794\n",
      "Epoch 19/50\n",
      "855229/855229 [==============================] - 37s 43us/sample - loss: 0.4265 - accuracy: 0.7912 - val_loss: 0.4243 - val_accuracy: 0.7914\n",
      "Epoch 20/50\n",
      "855229/855229 [==============================] - 37s 43us/sample - loss: 0.4528 - accuracy: 0.7892 - val_loss: 0.4903 - val_accuracy: 0.7876\n",
      "Epoch 21/50\n",
      "855229/855229 [==============================] - 37s 43us/sample - loss: 0.4884 - accuracy: 0.7797 - val_loss: 0.4772 - val_accuracy: 0.7781\n",
      "Epoch 22/50\n",
      "855229/855229 [==============================] - 37s 43us/sample - loss: 0.4497 - accuracy: 0.7814 - val_loss: 0.4405 - val_accuracy: 0.7812\n",
      "Epoch 23/50\n",
      "855229/855229 [==============================] - 37s 43us/sample - loss: 0.4407 - accuracy: 0.7805 - val_loss: 0.4504 - val_accuracy: 0.7699\n",
      "Epoch 24/50\n",
      "855229/855229 [==============================] - 37s 43us/sample - loss: 0.4508 - accuracy: 0.7725 - val_loss: 0.4489 - val_accuracy: 0.7744\n",
      "Epoch 25/50\n",
      "855229/855229 [==============================] - 37s 44us/sample - loss: 0.4613 - accuracy: 0.7659 - val_loss: 0.4784 - val_accuracy: 0.7523\n",
      "Epoch 26/50\n",
      "855229/855229 [==============================] - 38s 44us/sample - loss: 0.4656 - accuracy: 0.7613 - val_loss: 0.4588 - val_accuracy: 0.7656\n",
      "Epoch 27/50\n",
      "855229/855229 [==============================] - 39s 46us/sample - loss: 0.4556 - accuracy: 0.7685 - val_loss: 0.4522 - val_accuracy: 0.7714\n",
      "Epoch 28/50\n",
      "855229/855229 [==============================] - 38s 44us/sample - loss: 0.4536 - accuracy: 0.7688 - val_loss: 0.4885 - val_accuracy: 0.7407\n",
      "Epoch 29/50\n",
      "855229/855229 [==============================] - 37s 44us/sample - loss: 0.4671 - accuracy: 0.7583 - val_loss: 0.4773 - val_accuracy: 0.7544\n",
      "Epoch 30/50\n",
      "855229/855229 [==============================] - 37s 43us/sample - loss: 0.4639 - accuracy: 0.7615 - val_loss: 0.4600 - val_accuracy: 0.7637\n",
      "Epoch 31/50\n",
      "855229/855229 [==============================] - 37s 43us/sample - loss: 0.4571 - accuracy: 0.7654 - val_loss: 0.4627 - val_accuracy: 0.7598\n",
      "Epoch 32/50\n",
      "855229/855229 [==============================] - 37s 43us/sample - loss: 0.4666 - accuracy: 0.7589 - val_loss: 0.4875 - val_accuracy: 0.7447\n",
      "Epoch 33/50\n",
      "855229/855229 [==============================] - 37s 44us/sample - loss: 0.4754 - accuracy: 0.7547 - val_loss: 0.4700 - val_accuracy: 0.7583\n",
      "Epoch 34/50\n",
      "855229/855229 [==============================] - 37s 44us/sample - loss: 0.4665 - accuracy: 0.7606 - val_loss: 0.4643 - val_accuracy: 0.7621\n",
      "Epoch 35/50\n",
      "855229/855229 [==============================] - 37s 44us/sample - loss: 0.4613 - accuracy: 0.7645 - val_loss: 0.4631 - val_accuracy: 0.7628\n",
      "Epoch 36/50\n",
      "855229/855229 [==============================] - 38s 44us/sample - loss: 0.4588 - accuracy: 0.7665 - val_loss: 0.4591 - val_accuracy: 0.7663\n",
      "Epoch 37/50\n",
      "855229/855229 [==============================] - 37s 44us/sample - loss: 0.4556 - accuracy: 0.7688 - val_loss: 0.4566 - val_accuracy: 0.7668\n",
      "Epoch 38/50\n",
      "855229/855229 [==============================] - 37s 44us/sample - loss: 0.4664 - accuracy: 0.7604 - val_loss: 0.4637 - val_accuracy: 0.7611\n",
      "Epoch 39/50\n",
      "855229/855229 [==============================] - 37s 43us/sample - loss: 0.4607 - accuracy: 0.7640 - val_loss: 0.4609 - val_accuracy: 0.7630\n",
      "Epoch 40/50\n",
      "855229/855229 [==============================] - 37s 43us/sample - loss: 0.5139 - accuracy: 0.7204 - val_loss: 0.5121 - val_accuracy: 0.7206\n",
      "Epoch 41/50\n",
      "855229/855229 [==============================] - 38s 44us/sample - loss: 0.5026 - accuracy: 0.7277 - val_loss: 0.5001 - val_accuracy: 0.7306\n",
      "Epoch 42/50\n",
      "855229/855229 [==============================] - 37s 44us/sample - loss: 0.5087 - accuracy: 0.7274 - val_loss: 0.5168 - val_accuracy: 0.7240\n",
      "Epoch 43/50\n",
      "855229/855229 [==============================] - 37s 43us/sample - loss: 0.5010 - accuracy: 0.7361 - val_loss: 0.4954 - val_accuracy: 0.7388\n",
      "Epoch 44/50\n",
      "855229/855229 [==============================] - 37s 44us/sample - loss: 0.4988 - accuracy: 0.7349 - val_loss: 0.5042 - val_accuracy: 0.7285\n",
      "Epoch 45/50\n",
      "855229/855229 [==============================] - 37s 44us/sample - loss: 0.4968 - accuracy: 0.7352 - val_loss: 0.4940 - val_accuracy: 0.7380\n",
      "Epoch 46/50\n",
      "855229/855229 [==============================] - 37s 43us/sample - loss: 0.4891 - accuracy: 0.7421 - val_loss: 0.4878 - val_accuracy: 0.7432\n",
      "Epoch 47/50\n",
      "855229/855229 [==============================] - 37s 43us/sample - loss: 0.4841 - accuracy: 0.7463 - val_loss: 0.4835 - val_accuracy: 0.7459\n",
      "Epoch 48/50\n",
      "855229/855229 [==============================] - 37s 44us/sample - loss: 0.4804 - accuracy: 0.7491 - val_loss: 0.4806 - val_accuracy: 0.7484\n",
      "Epoch 49/50\n",
      "855229/855229 [==============================] - 38s 44us/sample - loss: 0.4776 - accuracy: 0.7513 - val_loss: 0.4777 - val_accuracy: 0.7511\n",
      "Epoch 50/50\n",
      "855229/855229 [==============================] - 37s 44us/sample - loss: 0.4749 - accuracy: 0.7534 - val_loss: 0.4753 - val_accuracy: 0.7531\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fdfc84c2860>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logdir = tempfile.mkdtemp()\n",
    "\n",
    "callbacks = [\n",
    "  tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "  tfmot.sparsity.keras.PruningSummaries(log_dir=logdir),\n",
    "]\n",
    "  \n",
    "model_for_pruning.fit(X_train, y_train,\n",
    "                  batch_size=batch_size, epochs=epochs, validation_split=validation_split,\n",
    "                  callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline test accuracy: 0.8394188\n",
      "Pruned test accuracy: 0.754209\n"
     ]
    }
   ],
   "source": [
    "_, model_for_pruning_accuracy = model_for_pruning.evaluate(\n",
    "   X_test, y_test, verbose=0)\n",
    "\n",
    "print('Baseline test accuracy:', baseline_model_accuracy) \n",
    "print('Pruned test accuracy:', model_for_pruning_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-8b4bea97dc69f387\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-8b4bea97dc69f387\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6007;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir={logdir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create 3x smaller models from pruning\n",
    "\n",
    "Both tfmot.sparsity.keras.strip_pruning and applying a standard compression algorithm (e.g. via gzip) are necessary to see the compression benefits of pruning.  \n",
    "\n",
    "First, create a compressible model for TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved pruned Keras model to: /tmp/tmp01ebsw3z.h5\n"
     ]
    }
   ],
   "source": [
    "model_for_export = tfmot.sparsity.keras.strip_pruning(model_for_pruning)\n",
    "\n",
    "_, pruned_keras_file = tempfile.mkstemp('.h5')\n",
    "tf.keras.models.save_model(model_for_export, pruned_keras_file, include_optimizer=False)\n",
    "print('Saved pruned Keras model to:', pruned_keras_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, create a compressible model for TFLite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'kernel'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-0052d0b2e15c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFLiteConverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_keras_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_for_export\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_new_converter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpruned_tflite_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpruned_tflite_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtempfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkstemp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.tflite'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/research_data/anaconda3/lib/python3.7/site-packages/tensorflow_core/lite/python/lite.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[0;32m/research_data/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/convert_to_constants.py\u001b[0m in \u001b[0;36mconvert_variables_to_constants_v2\u001b[0;34m(func, lower_control_flow)\u001b[0m\n",
      "\u001b[0;32m/research_data/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/convert_to_constants.py\u001b[0m in \u001b[0;36m_get_control_flow_function_data\u001b[0;34m(node_defs, tensor_data)\u001b[0m\n",
      "\u001b[0;32m/research_data/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/convert_to_constants.py\u001b[0m in \u001b[0;36mget_resource_type\u001b[0;34m(node_name)\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'kernel'"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\n",
    "converter.experimental_new_converter = True\n",
    "pruned_tflite_model = converter.convert()\n",
    "\n",
    "_, pruned_tflite_file = tempfile.mkstemp('.tflite')\n",
    "\n",
    "with open(pruned_tflite_file, 'wb') as f:\n",
    "    f.write(pruned_tflite_model)\n",
    "\n",
    "print('Saved pruned TFLite model to:', pruned_tflite_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a helper function to actually compress the models via gzip and measure the zipped size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gzipped_model_size(file):\n",
    "  # Returns size of gzipped model, in bytes.\n",
    "    import os\n",
    "    import zipfile\n",
    "\n",
    "    _, zipped_file = tempfile.mkstemp('.zip')\n",
    "    with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
    "        f.write(file)\n",
    "\n",
    "    return os.path.getsize(zipped_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Size of gzipped baseline Keras model: %.2f bytes\" % (get_gzipped_model_size(keras_file)))\n",
    "print(\"Size of gzipped pruned Keras model: %.2f bytes\" % (get_gzipped_model_size(pruned_keras_file)))\n",
    "print(\"Size of gzipped pruned TFlite model: %.2f bytes\" % (get_gzipped_model_size(pruned_tflite_file)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict(text_x):\n",
    "# \tx = np.zeros((1, maxlen), dtype=np.int)\n",
    "# \toffset = max(maxlen - len(text_x), 0)\n",
    "# \tfor t, char in enumerate(text_x):\n",
    "# \t    if t >= maxlen:\n",
    "# \t        break\n",
    "# \t    x[0, t + offset] = char_indices[char]\n",
    "#     pred = model.predict(x)\n",
    "# \treturn pred[0][0]\n",
    "\n",
    "\n",
    "# Like predict, but you pass in an array of URLs, and it is all\n",
    "# vectorized in one step, making it more efficient\n",
    "def predicts(text_X):\n",
    "    X = np.zeros((len(text_X), maxlen), dtype=np.int)\n",
    "    for i in range(len(text_X)):\n",
    "        offset = max(maxlen - len(text_X[i]), 0)\n",
    "        for t, char in enumerate(text_X[i]):\n",
    "            if t >= maxlen:\n",
    "                break\n",
    "            X[i, t + offset] = char_indices[char]\n",
    "    preds = [pred[0] for pred in model.predict(X)]\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation\n",
    "# define a threshold value so that values below threshold will be classified as false_positive\n",
    "threshold = 0.5\n",
    "\n",
    "def evaluate_model(positives, negatives_train, negatives_dev, negatives_test, threshold):\n",
    "    false_negatives = 0.0\n",
    "    preds = predicts(positives)\n",
    "    for pred in preds:\n",
    "        if pred <= threshold:\n",
    "            false_negatives += 1\n",
    "    print(false_negatives / len(positives), \"false negatives for positives set.\")\n",
    "\n",
    "    false_positives_train = 0.0\n",
    "    preds = predicts(negatives_train)\n",
    "    for pred in preds:\n",
    "        if pred > threshold:\n",
    "            false_positives_train += 1\n",
    "\n",
    "    false_positives_dev = 0.0\n",
    "    preds = predicts(negatives_dev)\n",
    "    for pred in preds:\n",
    "        if pred > threshold:\n",
    "            false_positives_dev += 1\n",
    "\n",
    "    false_positives_test = 0.0\n",
    "    preds = predicts(negatives_test)\n",
    "    for pred in preds:\n",
    "        if pred > threshold:\n",
    "            false_positives_test += 1\n",
    "\n",
    "    print(false_positives_train / len(negatives_train), \"false positive rate for negative train.\")\n",
    "    print(false_positives_dev / len(negatives_dev), \"false positive rate for negative dev.\")\n",
    "    print(false_positives_test / len(negatives_test), \"false positive rate for negative test.\")\n",
    "\n",
    "evaluate_model(positives, negatives_train, negatives_dev, negatives_test, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting predictions on negative_dev set to find a suitable threshold value.\n",
    "\n",
    "# defining the false positive rate which we can change.\n",
    "fp_rate = 0.01\n",
    "\n",
    "print(\"Getting threshold for fp_rate\", fp_rate)\n",
    "preds = predicts(negatives_dev)\n",
    "preds.sort()\n",
    "fp_index = math.ceil((len(negatives_dev) * (1 - fp_rate)))\n",
    "threshold = preds[fp_index]\n",
    "\n",
    "print(\"Using threshold\", threshold) \n",
    "\n",
    "evaluate_model(positives, negatives_train, negatives_dev, negatives_test, threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bloom Filter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Adapted from https://www.geeksforgeeks.org/bloom-filters-introduction-and-python-implementation/ \n",
    "\n",
    "import math \n",
    "import mmh3 \n",
    "from bitarray import bitarray \n",
    "\n",
    "class BloomFilter(object): \n",
    "\n",
    "\t''' \n",
    "\tClass for Bloom filter, using murmur3 hash function \n",
    "\t'''\n",
    "\n",
    "\tdef __init__(self, items_count,fp_prob): \n",
    "\t\t''' \n",
    "\t\titems_count : int \n",
    "\t\t\tNumber of items expected to be stored in bloom filter \n",
    "\t\tfp_prob : float \n",
    "\t\t\tFalse Positive probability in decimal \n",
    "\t\t'''\n",
    "\t\t# False posible probability in decimal \n",
    "\t\tself.fp_prob = fp_prob \n",
    "\n",
    "\t\t# Size of bit array to use \n",
    "\t\tself.size = self.get_size(items_count,fp_prob) \n",
    "\n",
    "\t\t# number of hash functions to use \n",
    "\t\tself.hash_count = self.get_hash_count(self.size,items_count) \n",
    "\n",
    "\t\t# Bit array of given size \n",
    "\t\tself.bit_array = bitarray(self.size) \n",
    "        \n",
    "        # Return the size of bitarray in bytes\n",
    "        self.byte_size = self.bit_array.tobyte()\n",
    "\n",
    "\t\t# initialize all bits as 0 \n",
    "\t\tself.bit_array.setall(0) \n",
    "\n",
    "\tdef add(self, item): \n",
    "\t\t''' \n",
    "\t\tAdd an item in the filter \n",
    "\t\t'''\n",
    "\t\tdigests = [] \n",
    "\t\tfor i in range(self.hash_count): \n",
    "\n",
    "\t\t\t# create digest for given item. \n",
    "\t\t\t# i work as seed to mmh3.hash() function \n",
    "\t\t\t# With different seed, digest created is different \n",
    "\t\t\tdigest = mmh3.hash(item,i) % self.size \n",
    "\t\t\tdigests.append(digest) \n",
    "\n",
    "\t\t\t# set the bit True in bit_array \n",
    "\t\t\tself.bit_array[digest] = True\n",
    "\n",
    "\tdef check(self, item): \n",
    "\t\t''' \n",
    "\t\tCheck for existence of an item in filter \n",
    "\t\t'''\n",
    "\t\tfor i in range(self.hash_count): \n",
    "\t\t\tdigest = mmh3.hash(item,i) % self.size \n",
    "\t\t\tif self.bit_array[digest] == False: \n",
    "\n",
    "\t\t\t\t# if any of bit is False then,its not present \n",
    "\t\t\t\t# in filter \n",
    "\t\t\t\t# else there is probability that it exist \n",
    "\t\t\t\treturn False\n",
    "\t\treturn True\n",
    "\n",
    "\t@classmethod\n",
    "\tdef get_size(self,n,p): \n",
    "\t\t''' \n",
    "\t\tReturn the size of bit array(m) to used using \n",
    "\t\tfollowing formula \n",
    "\t\tm = -(n * lg(p)) / (lg(2)^2) \n",
    "\t\tn : int \n",
    "\t\t\tnumber of items expected to be stored in filter \n",
    "\t\tp : float \n",
    "\t\t\tFalse Positive probability in decimal \n",
    "\t\t'''\n",
    "\t\tm = -(n * math.log(p))/(math.log(2)**2) \n",
    "\t\treturn int(m)\n",
    "    \n",
    "\t@classmethod\n",
    "\tdef get_hash_count(self, m, n): \n",
    "\t\t''' \n",
    "\t\tReturn the hash function(k) to be used using \n",
    "\t\tfollowing formula \n",
    "\t\tk = (m/n) * lg(2) \n",
    "\n",
    "\t\tm : int \n",
    "\t\t\tsize of bit array \n",
    "\t\tn : int \n",
    "\t\t\tnumber of items expected to be stored in filter \n",
    "\t\t'''\n",
    "\t\tk = (m/n) * math.log(2) \n",
    "\t\treturn int(k) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Deep Bloom Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the threshold value\n",
    "fp_rate = 0.01\n",
    "print(\"Getting threshold for fp_rate\", fp_rate)\n",
    "preds = predicts(negatives_dev)\n",
    "preds.sort()\n",
    "fp_index = math.ceil((len(negatives_dev) * (1 - fp_rate/2)))\n",
    "threshold = preds[fp_index]\n",
    "print(\"The threhold value to use is:\", threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bloom_filter(data):\n",
    "    print(\"Creating bloom filter\")\n",
    "    false_negatives = []\n",
    "    # calling the predicts function \n",
    "    preds = predicts(data)\n",
    "    for i in range(len(data)):\n",
    "        if preds[i] <= threshold:\n",
    "            false_negatives.append(data[i])\n",
    "    print(\"Number of false negatives at bloom time\", len(false_negatives))\n",
    "    bloom_filter = BloomFilter(len(false_negatives), fp_rate / 2)\n",
    "    for fn in false_negatives:\n",
    "        bloom_filter.add(fn)\n",
    "    print(\"Created bloom filter\")\n",
    "    return bloom_filter\n",
    "\n",
    "bloom_filter = create_bloom_filter(positives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict the output from the machine learning model\n",
    "def predict(text_x):\n",
    "    x = np.zeros((1, maxlen), dtype=np.int)\n",
    "    offset = max(maxlen - len(text_x), 0)\n",
    "    for t, char in enumerate(text_x):\n",
    "        if t >= maxlen:\n",
    "            break\n",
    "        x[0, t + offset] = char_indices[char]\n",
    "    pred = model.predict(x)\n",
    "    return pred[0][0]\n",
    "\n",
    "\n",
    "def check_item(item):\n",
    "    if predict(item) > threshold:\n",
    "        return True\n",
    "    return bloom_filter.check(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Deep Bloom Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Bloom filter bits needed\", bloom_filter.size)\n",
    "print(\"Bloom fiter size in bytes\", bloom_filter.byte_size)\n",
    "print(\"Hash functions needed\", bloom_filter.hash_count)\n",
    "    \n",
    "false_positives = 0.0\n",
    "for neg in negatives_test:\n",
    "    if check_item(neg):\n",
    "        false_positives += 1\n",
    "print(\"Test false positive rate: \", str(false_positives / len(negatives_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
